<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">


<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    
    <title>GPy.core.gp &mdash; GPy  documentation</title>
    
    <link rel="stylesheet" href="../../../_static//default.css" type="text/css" />
    <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
    
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    '../../../',
        VERSION:     '',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true
      };
    </script>
    <script type="text/javascript" src="../../../_static/jquery.js"></script>
    <script type="text/javascript" src="../../../_static/underscore.js"></script>
    <script type="text/javascript" src="../../../_static/doctools.js"></script>
    <link rel="top" title="GPy  documentation" href="../../../index.html" />
    <link rel="up" title="GPy" href="../../GPy.html" /> 
  </head>
  <body role="document">
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../../../genindex.html" title="General Index"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="../../../py-modindex.html" title="Python Module Index"
             >modules</a> |</li>
        <li class="nav-item nav-item-0"><a href="../../../index.html">GPy  documentation</a> &raquo;</li>
          <li class="nav-item nav-item-1"><a href="../../index.html" >Module code</a> &raquo;</li>
          <li class="nav-item nav-item-2"><a href="../../GPy.html" accesskey="U">GPy</a> &raquo;</li> 
      </ul>
    </div>  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <h1>Source code for GPy.core.gp</h1><div class="highlight"><pre>
<span class="c"># Copyright (c) 2012-2014, GPy authors (see AUTHORS.txt).</span>
<span class="c"># Licensed under the BSD 3-clause license (see LICENSE.txt)</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">sys</span>
<span class="kn">from</span> <span class="nn">..</span> <span class="kn">import</span> <span class="n">kern</span>
<span class="kn">from</span> <span class="nn">model</span> <span class="kn">import</span> <span class="n">Model</span>
<span class="kn">from</span> <span class="nn">parameterization</span> <span class="kn">import</span> <span class="n">ObsAr</span>
<span class="kn">from</span> <span class="nn">..</span> <span class="kn">import</span> <span class="n">likelihoods</span>
<span class="kn">from</span> <span class="nn">..inference.latent_function_inference</span> <span class="kn">import</span> <span class="n">exact_gaussian_inference</span><span class="p">,</span> <span class="n">expectation_propagation</span>
<span class="kn">from</span> <span class="nn">parameterization.variational</span> <span class="kn">import</span> <span class="n">VariationalPosterior</span>

<span class="kn">import</span> <span class="nn">logging</span>
<span class="kn">from</span> <span class="nn">GPy.util.normalizer</span> <span class="kn">import</span> <span class="n">MeanNorm</span>
<span class="n">logger</span> <span class="o">=</span> <span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">(</span><span class="s">&quot;GP&quot;</span><span class="p">)</span>

<div class="viewcode-block" id="GP"><a class="viewcode-back" href="../../../GPy.core.html#GPy.core.gp.GP">[docs]</a><span class="k">class</span> <span class="nc">GP</span><span class="p">(</span><span class="n">Model</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    General purpose Gaussian process model</span>

<span class="sd">    :param X: input observations</span>
<span class="sd">    :param Y: output observations</span>
<span class="sd">    :param kernel: a GPy kernel, defaults to rbf+white</span>
<span class="sd">    :param likelihood: a GPy likelihood</span>
<span class="sd">    :param inference_method: The :class:`~GPy.inference.latent_function_inference.LatentFunctionInference` inference method to use for this GP</span>
<span class="sd">    :rtype: model object</span>
<span class="sd">    :param Norm normalizer:</span>
<span class="sd">        normalize the outputs Y.</span>
<span class="sd">        Prediction will be un-normalized using this normalizer.</span>
<span class="sd">        If normalizer is None, we will normalize using MeanNorm.</span>
<span class="sd">        If normalizer is False, no normalization will be done.</span>

<span class="sd">    .. Note:: Multiple independent outputs are allowed using columns of Y</span>


<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">kernel</span><span class="p">,</span> <span class="n">likelihood</span><span class="p">,</span> <span class="n">inference_method</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s">&#39;gp&#39;</span><span class="p">,</span> <span class="n">Y_metadata</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">normalizer</span><span class="o">=</span><span class="bp">False</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">GP</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">__init__</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>

        <span class="k">assert</span> <span class="n">X</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">2</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="p">(</span><span class="n">ObsAr</span><span class="p">,</span> <span class="n">VariationalPosterior</span><span class="p">)):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">X</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">X</span> <span class="o">=</span> <span class="n">ObsAr</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">num_data</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_dim</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span>

        <span class="k">assert</span> <span class="n">Y</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">2</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s">&quot;initializing Y&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">normalizer</span> <span class="ow">is</span> <span class="bp">True</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">normalizer</span> <span class="o">=</span> <span class="n">MeanNorm</span><span class="p">()</span>
        <span class="k">elif</span> <span class="n">normalizer</span> <span class="ow">is</span> <span class="bp">False</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">normalizer</span> <span class="o">=</span> <span class="bp">None</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">normalizer</span> <span class="o">=</span> <span class="n">normalizer</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">normalizer</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">normalizer</span><span class="o">.</span><span class="n">scale_by</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">Y_normalized</span> <span class="o">=</span> <span class="n">ObsAr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">normalizer</span><span class="o">.</span><span class="n">normalize</span><span class="p">(</span><span class="n">Y</span><span class="p">))</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">Y</span> <span class="o">=</span> <span class="n">Y</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">Y</span> <span class="o">=</span> <span class="n">ObsAr</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">Y_normalized</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">Y</span>

        <span class="k">assert</span> <span class="n">Y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_data</span>
        <span class="n">_</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_dim</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">Y</span><span class="o">.</span><span class="n">shape</span>

        <span class="c">#TODO: check the type of this is okay?</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">Y_metadata</span> <span class="o">=</span> <span class="n">Y_metadata</span>

        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">kernel</span><span class="p">,</span> <span class="n">kern</span><span class="o">.</span><span class="n">Kern</span><span class="p">)</span>
        <span class="c">#assert self.input_dim == kernel.input_dim</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">kern</span> <span class="o">=</span> <span class="n">kernel</span>

        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">likelihood</span><span class="p">,</span> <span class="n">likelihoods</span><span class="o">.</span><span class="n">Likelihood</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">likelihood</span> <span class="o">=</span> <span class="n">likelihood</span>

        <span class="c">#find a sensible inference method</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s">&quot;initializing inference method&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">inference_method</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">likelihood</span><span class="p">,</span> <span class="n">likelihoods</span><span class="o">.</span><span class="n">Gaussian</span><span class="p">)</span> <span class="ow">or</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">likelihood</span><span class="p">,</span> <span class="n">likelihoods</span><span class="o">.</span><span class="n">MixedNoise</span><span class="p">):</span>
                <span class="n">inference_method</span> <span class="o">=</span> <span class="n">exact_gaussian_inference</span><span class="o">.</span><span class="n">ExactGaussianInference</span><span class="p">()</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">inference_method</span> <span class="o">=</span> <span class="n">expectation_propagation</span><span class="o">.</span><span class="n">EP</span><span class="p">()</span>
                <span class="k">print</span> <span class="s">&quot;defaulting to &quot;</span><span class="p">,</span> <span class="n">inference_method</span><span class="p">,</span> <span class="s">&quot;for latent function inference&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">inference_method</span> <span class="o">=</span> <span class="n">inference_method</span>

        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s">&quot;adding kernel and likelihood as parameters&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">link_parameter</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">kern</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">link_parameter</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">likelihood</span><span class="p">)</span>

<div class="viewcode-block" id="GP.set_XY"><a class="viewcode-back" href="../../../GPy.core.html#GPy.core.gp.GP.set_XY">[docs]</a>    <span class="k">def</span> <span class="nf">set_XY</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">Y</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Set the input / output data of the model</span>
<span class="sd">        This is useful if we wish to change our existing data but maintain the same model</span>

<span class="sd">        :param X: input observations</span>
<span class="sd">        :type X: np.ndarray</span>
<span class="sd">        :param Y: output observations</span>
<span class="sd">        :type Y: np.ndarray</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">update_model</span><span class="p">(</span><span class="bp">False</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">Y</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">normalizer</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">normalizer</span><span class="o">.</span><span class="n">scale_by</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">Y_normalized</span> <span class="o">=</span> <span class="n">ObsAr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">normalizer</span><span class="o">.</span><span class="n">normalize</span><span class="p">(</span><span class="n">Y</span><span class="p">))</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">Y</span> <span class="o">=</span> <span class="n">Y</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">Y</span> <span class="o">=</span> <span class="n">ObsAr</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">Y_normalized</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">Y</span>
        <span class="k">if</span> <span class="n">X</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">X</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">:</span>
                <span class="c"># LVM models</span>
                <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">X</span><span class="p">,</span> <span class="n">VariationalPosterior</span><span class="p">):</span>
                    <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">X</span><span class="p">)),</span> <span class="s">&quot;The given X must have the same type as the X in the model!&quot;</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">unlink_parameter</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">X</span><span class="p">)</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">X</span> <span class="o">=</span> <span class="n">X</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">link_parameters</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">X</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">unlink_parameter</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">X</span><span class="p">)</span>
                    <span class="kn">from</span> <span class="nn">..core</span> <span class="kn">import</span> <span class="n">Param</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">X</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="s">&#39;latent mean&#39;</span><span class="p">,</span><span class="n">X</span><span class="p">)</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">link_parameters</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">X</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">X</span> <span class="o">=</span> <span class="n">ObsAr</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">update_model</span><span class="p">(</span><span class="bp">True</span><span class="p">)</span>
</div>
<div class="viewcode-block" id="GP.set_X"><a class="viewcode-back" href="../../../GPy.core.html#GPy.core.gp.GP.set_X">[docs]</a>    <span class="k">def</span> <span class="nf">set_X</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">X</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Set the input data of the model</span>

<span class="sd">        :param X: input observations</span>
<span class="sd">        :type X: np.ndarray</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">set_XY</span><span class="p">(</span><span class="n">X</span><span class="o">=</span><span class="n">X</span><span class="p">)</span>
</div>
<div class="viewcode-block" id="GP.set_Y"><a class="viewcode-back" href="../../../GPy.core.html#GPy.core.gp.GP.set_Y">[docs]</a>    <span class="k">def</span> <span class="nf">set_Y</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">Y</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Set the output data of the model</span>

<span class="sd">        :param X: output observations</span>
<span class="sd">        :type X: np.ndarray</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">set_XY</span><span class="p">(</span><span class="n">Y</span><span class="o">=</span><span class="n">Y</span><span class="p">)</span>
</div>
<div class="viewcode-block" id="GP.parameters_changed"><a class="viewcode-back" href="../../../GPy.core.html#GPy.core.gp.GP.parameters_changed">[docs]</a>    <span class="k">def</span> <span class="nf">parameters_changed</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Method that is called upon any changes to :class:`~GPy.core.parameterization.param.Param` variables within the model.</span>
<span class="sd">        In particular in the GP class this method reperforms inference, recalculating the posterior and log marginal likelihood and gradients of the model</span>

<span class="sd">        .. warning::</span>
<span class="sd">            This method is not designed to be called manually, the framework is set up to automatically call this method upon changes to parameters, if you call</span>
<span class="sd">            this method yourself, there may be unexpected consequences.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">posterior</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_log_marginal_likelihood</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">grad_dict</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">inference_method</span><span class="o">.</span><span class="n">inference</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">kern</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">X</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">likelihood</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">Y_normalized</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">Y_metadata</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">likelihood</span><span class="o">.</span><span class="n">update_gradients</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">grad_dict</span><span class="p">[</span><span class="s">&#39;dL_dthetaL&#39;</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">kern</span><span class="o">.</span><span class="n">update_gradients_full</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">grad_dict</span><span class="p">[</span><span class="s">&#39;dL_dK&#39;</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">X</span><span class="p">)</span>
</div>
<div class="viewcode-block" id="GP.log_likelihood"><a class="viewcode-back" href="../../../GPy.core.html#GPy.core.gp.GP.log_likelihood">[docs]</a>    <span class="k">def</span> <span class="nf">log_likelihood</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        The log marginal likelihood of the model, :math:`p(\mathbf{y})`, this is the objective function of the model being optimised</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_log_marginal_likelihood</span>
</div>
    <span class="k">def</span> <span class="nf">_raw_predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">_Xnew</span><span class="p">,</span> <span class="n">full_cov</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">kern</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        For making predictions, does not account for normalization or likelihood</span>

<span class="sd">        full_cov is a boolean which defines whether the full covariance matrix</span>
<span class="sd">        of the prediction is computed. If full_cov is False (default), only the</span>
<span class="sd">        diagonal of the covariance is returned.</span>

<span class="sd">        .. math::</span>
<span class="sd">            p(f*|X*, X, Y) = \int^{\inf}_{\inf} p(f*|f,X*)p(f|X,Y) df</span>
<span class="sd">                        = N(f*| K_{x*x}(K_{xx} + \Sigma)^{-1}Y, K_{x*x*} - K_{xx*}(K_{xx} + \Sigma)^{-1}K_{xx*}</span>
<span class="sd">            \Sigma := \texttt{Likelihood.variance / Approximate likelihood covariance}</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">kern</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
            <span class="n">kern</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">kern</span>

        <span class="n">Kx</span> <span class="o">=</span> <span class="n">kern</span><span class="o">.</span><span class="n">K</span><span class="p">(</span><span class="n">_Xnew</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">X</span><span class="p">)</span><span class="o">.</span><span class="n">T</span>
        <span class="n">WiKx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">posterior</span><span class="o">.</span><span class="n">woodbury_inv</span><span class="p">,</span> <span class="n">Kx</span><span class="p">)</span>
        <span class="n">mu</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">Kx</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">posterior</span><span class="o">.</span><span class="n">woodbury_vector</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">full_cov</span><span class="p">:</span>
            <span class="n">Kxx</span> <span class="o">=</span> <span class="n">kern</span><span class="o">.</span><span class="n">K</span><span class="p">(</span><span class="n">_Xnew</span><span class="p">)</span>
            <span class="n">var</span> <span class="o">=</span> <span class="n">Kxx</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">Kx</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">WiKx</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">Kxx</span> <span class="o">=</span> <span class="n">kern</span><span class="o">.</span><span class="n">Kdiag</span><span class="p">(</span><span class="n">_Xnew</span><span class="p">)</span>
            <span class="n">var</span> <span class="o">=</span> <span class="n">Kxx</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">WiKx</span><span class="o">*</span><span class="n">Kx</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
            <span class="n">var</span> <span class="o">=</span> <span class="n">var</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

        <span class="c">#force mu to be a column vector</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">mu</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">==</span><span class="mi">1</span><span class="p">:</span> <span class="n">mu</span> <span class="o">=</span> <span class="n">mu</span><span class="p">[:,</span><span class="bp">None</span><span class="p">]</span>
        <span class="k">return</span> <span class="n">mu</span><span class="p">,</span> <span class="n">var</span>

<div class="viewcode-block" id="GP.predict"><a class="viewcode-back" href="../../../GPy.core.html#GPy.core.gp.GP.predict">[docs]</a>    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">Xnew</span><span class="p">,</span> <span class="n">full_cov</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">Y_metadata</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">kern</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Predict the function(s) at the new point(s) Xnew.</span>

<span class="sd">        :param Xnew: The points at which to make a prediction</span>
<span class="sd">        :type Xnew: np.ndarray (Nnew x self.input_dim)</span>
<span class="sd">        :param full_cov: whether to return the full covariance matrix, or just</span>
<span class="sd">                         the diagonal</span>
<span class="sd">        :type full_cov: bool</span>
<span class="sd">        :param Y_metadata: metadata about the predicting point to pass to the likelihood</span>
<span class="sd">        :param kern: The kernel to use for prediction (defaults to the model</span>
<span class="sd">                     kern). this is useful for examining e.g. subprocesses.</span>
<span class="sd">        :returns: (mean, var, lower_upper):</span>
<span class="sd">            mean: posterior mean, a Numpy array, Nnew x self.input_dim</span>
<span class="sd">            var: posterior variance, a Numpy array, Nnew x 1 if full_cov=False, Nnew x Nnew otherwise</span>
<span class="sd">            lower_upper: lower and upper boundaries of the 95% confidence intervals, Numpy arrays,  Nnew x self.input_dim</span>

<span class="sd">           If full_cov and self.input_dim &gt; 1, the return shape of var is Nnew x Nnew x self.input_dim. If self.input_dim == 1, the return shape is Nnew x Nnew.</span>
<span class="sd">           This is to allow for different normalizations of the output dimensions.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c">#predict the latent function values</span>
        <span class="n">mu</span><span class="p">,</span> <span class="n">var</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_raw_predict</span><span class="p">(</span><span class="n">Xnew</span><span class="p">,</span> <span class="n">full_cov</span><span class="o">=</span><span class="n">full_cov</span><span class="p">,</span> <span class="n">kern</span><span class="o">=</span><span class="n">kern</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">normalizer</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
            <span class="n">mu</span><span class="p">,</span> <span class="n">var</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">normalizer</span><span class="o">.</span><span class="n">inverse_mean</span><span class="p">(</span><span class="n">mu</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">normalizer</span><span class="o">.</span><span class="n">inverse_variance</span><span class="p">(</span><span class="n">var</span><span class="p">)</span>

        <span class="c"># now push through likelihood</span>
        <span class="n">mean</span><span class="p">,</span> <span class="n">var</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">likelihood</span><span class="o">.</span><span class="n">predictive_values</span><span class="p">(</span><span class="n">mu</span><span class="p">,</span> <span class="n">var</span><span class="p">,</span> <span class="n">full_cov</span><span class="p">,</span> <span class="n">Y_metadata</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">mean</span><span class="p">,</span> <span class="n">var</span>
</div>
<div class="viewcode-block" id="GP.predict_quantiles"><a class="viewcode-back" href="../../../GPy.core.html#GPy.core.gp.GP.predict_quantiles">[docs]</a>    <span class="k">def</span> <span class="nf">predict_quantiles</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">quantiles</span><span class="o">=</span><span class="p">(</span><span class="mf">2.5</span><span class="p">,</span> <span class="mf">97.5</span><span class="p">),</span> <span class="n">Y_metadata</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Get the predictive quantiles around the prediction at X</span>

<span class="sd">        :param X: The points at which to make a prediction</span>
<span class="sd">        :type X: np.ndarray (Xnew x self.input_dim)</span>
<span class="sd">        :param quantiles: tuple of quantiles, default is (2.5, 97.5) which is the 95% interval</span>
<span class="sd">        :type quantiles: tuple</span>
<span class="sd">        :returns: list of quantiles for each X and predictive quantiles for interval combination</span>
<span class="sd">        :rtype: [np.ndarray (Xnew x self.input_dim), np.ndarray (Xnew x self.input_dim)]</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">m</span><span class="p">,</span> <span class="n">v</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_raw_predict</span><span class="p">(</span><span class="n">X</span><span class="p">,</span>  <span class="n">full_cov</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">normalizer</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
            <span class="n">m</span><span class="p">,</span> <span class="n">v</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">normalizer</span><span class="o">.</span><span class="n">inverse_mean</span><span class="p">(</span><span class="n">m</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">normalizer</span><span class="o">.</span><span class="n">inverse_variance</span><span class="p">(</span><span class="n">v</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">likelihood</span><span class="o">.</span><span class="n">predictive_quantiles</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">quantiles</span><span class="p">,</span> <span class="n">Y_metadata</span><span class="p">)</span>
</div>
<div class="viewcode-block" id="GP.predictive_gradients"><a class="viewcode-back" href="../../../GPy.core.html#GPy.core.gp.GP.predictive_gradients">[docs]</a>    <span class="k">def</span> <span class="nf">predictive_gradients</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">Xnew</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Compute the derivatives of the latent function with respect to X*</span>

<span class="sd">        Given a set of points at which to predict X* (size [N*,Q]), compute the</span>
<span class="sd">        derivatives of the mean and variance. Resulting arrays are sized:</span>
<span class="sd">         dmu_dX* -- [N*, Q ,D], where D is the number of output in this GP (usually one).</span>

<span class="sd">         dv_dX*  -- [N*, Q],    (since all outputs have the same variance)</span>
<span class="sd">        :param X: The points at which to get the predictive gradients</span>
<span class="sd">        :type X: np.ndarray (Xnew x self.input_dim)</span>
<span class="sd">        :returns: dmu_dX, dv_dX</span>
<span class="sd">        :rtype: [np.ndarray (N*, Q ,D), np.ndarray (N*,Q) ]</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">dmu_dX</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">((</span><span class="n">Xnew</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">Xnew</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span><span class="bp">self</span><span class="o">.</span><span class="n">output_dim</span><span class="p">))</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">output_dim</span><span class="p">):</span>
            <span class="n">dmu_dX</span><span class="p">[:,:,</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">kern</span><span class="o">.</span><span class="n">gradients_X</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">posterior</span><span class="o">.</span><span class="n">woodbury_vector</span><span class="p">[:,</span><span class="n">i</span><span class="p">:</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">Xnew</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">X</span><span class="p">)</span>

        <span class="c"># gradients wrt the diagonal part k_{xx}</span>
        <span class="n">dv_dX</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">kern</span><span class="o">.</span><span class="n">gradients_X</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">Xnew</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span> <span class="n">Xnew</span><span class="p">)</span>
        <span class="c">#grads wrt &#39;Schur&#39; part K_{xf}K_{ff}^{-1}K_{fx}</span>
        <span class="n">alpha</span> <span class="o">=</span> <span class="o">-</span><span class="mf">2.</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">kern</span><span class="o">.</span><span class="n">K</span><span class="p">(</span><span class="n">Xnew</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">X</span><span class="p">),</span><span class="bp">self</span><span class="o">.</span><span class="n">posterior</span><span class="o">.</span><span class="n">woodbury_inv</span><span class="p">)</span>
        <span class="n">dv_dX</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">kern</span><span class="o">.</span><span class="n">gradients_X</span><span class="p">(</span><span class="n">alpha</span><span class="p">,</span> <span class="n">Xnew</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">X</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">dmu_dX</span><span class="p">,</span> <span class="n">dv_dX</span>

</div>
<div class="viewcode-block" id="GP.posterior_samples_f"><a class="viewcode-back" href="../../../GPy.core.html#GPy.core.gp.GP.posterior_samples_f">[docs]</a>    <span class="k">def</span> <span class="nf">posterior_samples_f</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">X</span><span class="p">,</span><span class="n">size</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">full_cov</span><span class="o">=</span><span class="bp">True</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Samples the posterior GP at the points X.</span>

<span class="sd">        :param X: The points at which to take the samples.</span>
<span class="sd">        :type X: np.ndarray (Nnew x self.input_dim)</span>
<span class="sd">        :param size: the number of a posteriori samples.</span>
<span class="sd">        :type size: int.</span>
<span class="sd">        :param full_cov: whether to return the full covariance matrix, or just the diagonal.</span>
<span class="sd">        :type full_cov: bool.</span>
<span class="sd">        :returns: Ysim: set of simulations</span>
<span class="sd">        :rtype: np.ndarray (N x samples)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">m</span><span class="p">,</span> <span class="n">v</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_raw_predict</span><span class="p">(</span><span class="n">X</span><span class="p">,</span>  <span class="n">full_cov</span><span class="o">=</span><span class="n">full_cov</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">normalizer</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
            <span class="n">m</span><span class="p">,</span> <span class="n">v</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">normalizer</span><span class="o">.</span><span class="n">inverse_mean</span><span class="p">(</span><span class="n">m</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">normalizer</span><span class="o">.</span><span class="n">inverse_variance</span><span class="p">(</span><span class="n">v</span><span class="p">)</span>
        <span class="n">v</span> <span class="o">=</span> <span class="n">v</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">size</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">v</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">==</span><span class="mi">3</span> <span class="k">else</span> <span class="n">v</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">full_cov</span><span class="p">:</span>
            <span class="n">Ysim</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">multivariate_normal</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">flatten</span><span class="p">(),</span> <span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">v</span><span class="o">.</span><span class="n">flatten</span><span class="p">()),</span> <span class="n">size</span><span class="p">)</span><span class="o">.</span><span class="n">T</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">Ysim</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">multivariate_normal</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">flatten</span><span class="p">(),</span> <span class="n">v</span><span class="p">,</span> <span class="n">size</span><span class="p">)</span><span class="o">.</span><span class="n">T</span>

        <span class="k">return</span> <span class="n">Ysim</span>
</div>
<div class="viewcode-block" id="GP.posterior_samples"><a class="viewcode-back" href="../../../GPy.core.html#GPy.core.gp.GP.posterior_samples">[docs]</a>    <span class="k">def</span> <span class="nf">posterior_samples</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">full_cov</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">Y_metadata</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Samples the posterior GP at the points X.</span>

<span class="sd">        :param X: the points at which to take the samples.</span>
<span class="sd">        :type X: np.ndarray (Nnew x self.input_dim.)</span>
<span class="sd">        :param size: the number of a posteriori samples.</span>
<span class="sd">        :type size: int.</span>
<span class="sd">        :param full_cov: whether to return the full covariance matrix, or just the diagonal.</span>
<span class="sd">        :type full_cov: bool.</span>
<span class="sd">        :param noise_model: for mixed noise likelihood, the noise model to use in the samples.</span>
<span class="sd">        :type noise_model: integer.</span>
<span class="sd">        :returns: Ysim: set of simulations, a Numpy array (N x samples).</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">Ysim</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">posterior_samples_f</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">full_cov</span><span class="o">=</span><span class="n">full_cov</span><span class="p">)</span>
        <span class="n">Ysim</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">likelihood</span><span class="o">.</span><span class="n">samples</span><span class="p">(</span><span class="n">Ysim</span><span class="p">,</span> <span class="n">Y_metadata</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">Ysim</span>
</div>
<div class="viewcode-block" id="GP.plot_f"><a class="viewcode-back" href="../../../GPy.core.html#GPy.core.gp.GP.plot_f">[docs]</a>    <span class="k">def</span> <span class="nf">plot_f</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">plot_limits</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">which_data_rows</span><span class="o">=</span><span class="s">&#39;all&#39;</span><span class="p">,</span>
        <span class="n">which_data_ycols</span><span class="o">=</span><span class="s">&#39;all&#39;</span><span class="p">,</span> <span class="n">fixed_inputs</span><span class="o">=</span><span class="p">[],</span>
        <span class="n">levels</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">samples</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">fignum</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">resolution</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
        <span class="n">plot_raw</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
        <span class="n">linecol</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span><span class="n">fillcol</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">Y_metadata</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">data_symbol</span><span class="o">=</span><span class="s">&#39;kx&#39;</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Plot the GP&#39;s view of the world, where the data is normalized and before applying a likelihood.</span>
<span class="sd">        This is a call to plot with plot_raw=True.</span>
<span class="sd">        Data will not be plotted in this, as the GP&#39;s view of the world</span>
<span class="sd">        may live in another space, or units then the data.</span>

<span class="sd">        Can plot only part of the data and part of the posterior functions</span>
<span class="sd">        using which_data_rowsm which_data_ycols.</span>

<span class="sd">        :param plot_limits: The limits of the plot. If 1D [xmin,xmax], if 2D [[xmin,ymin],[xmax,ymax]]. Defaluts to data limits</span>
<span class="sd">        :type plot_limits: np.array</span>
<span class="sd">        :param which_data_rows: which of the training data to plot (default all)</span>
<span class="sd">        :type which_data_rows: &#39;all&#39; or a slice object to slice model.X, model.Y</span>
<span class="sd">        :param which_data_ycols: when the data has several columns (independant outputs), only plot these</span>
<span class="sd">        :type which_data_ycols: &#39;all&#39; or a list of integers</span>
<span class="sd">        :param fixed_inputs: a list of tuple [(i,v), (i,v)...], specifying that input index i should be set to value v.</span>
<span class="sd">        :type fixed_inputs: a list of tuples</span>
<span class="sd">        :param resolution: the number of intervals to sample the GP on. Defaults to 200 in 1D and 50 (a 50x50 grid) in 2D</span>
<span class="sd">        :type resolution: int</span>
<span class="sd">        :param levels: number of levels to plot in a contour plot.</span>
<span class="sd">        :param levels: for 2D plotting, the number of contour levels to use is ax is None, create a new figure</span>
<span class="sd">        :type levels: int</span>
<span class="sd">        :param samples: the number of a posteriori samples to plot</span>
<span class="sd">        :type samples: int</span>
<span class="sd">        :param fignum: figure to plot on.</span>
<span class="sd">        :type fignum: figure number</span>
<span class="sd">        :param ax: axes to plot on.</span>
<span class="sd">        :type ax: axes handle</span>
<span class="sd">        :param linecol: color of line to plot [Tango.colorsHex[&#39;darkBlue&#39;]]</span>
<span class="sd">        :type linecol: color either as Tango.colorsHex object or character (&#39;r&#39; is red, &#39;g&#39; is green) as is standard in matplotlib</span>
<span class="sd">        :param fillcol: color of fill [Tango.colorsHex[&#39;lightBlue&#39;]]</span>
<span class="sd">        :type fillcol: color either as Tango.colorsHex object or character (&#39;r&#39; is red, &#39;g&#39; is green) as is standard in matplotlib</span>
<span class="sd">        :param Y_metadata: additional data associated with Y which may be needed</span>
<span class="sd">        :type Y_metadata: dict</span>
<span class="sd">        :param data_symbol: symbol as used matplotlib, by default this is a black cross (&#39;kx&#39;)</span>
<span class="sd">        :type data_symbol: color either as Tango.colorsHex object or character (&#39;r&#39; is red, &#39;g&#39; is green) alongside marker type, as is standard in matplotlib.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">assert</span> <span class="s">&quot;matplotlib&quot;</span> <span class="ow">in</span> <span class="n">sys</span><span class="o">.</span><span class="n">modules</span><span class="p">,</span> <span class="s">&quot;matplotlib package has not been imported.&quot;</span>
        <span class="kn">from</span> <span class="nn">..plotting.matplot_dep</span> <span class="kn">import</span> <span class="n">models_plots</span>
        <span class="n">kw</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">if</span> <span class="n">linecol</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
            <span class="n">kw</span><span class="p">[</span><span class="s">&#39;linecol&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">linecol</span>
        <span class="k">if</span> <span class="n">fillcol</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
            <span class="n">kw</span><span class="p">[</span><span class="s">&#39;fillcol&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">fillcol</span>
        <span class="k">return</span> <span class="n">models_plots</span><span class="o">.</span><span class="n">plot_fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">plot_limits</span><span class="p">,</span> <span class="n">which_data_rows</span><span class="p">,</span>
                                     <span class="n">which_data_ycols</span><span class="p">,</span> <span class="n">fixed_inputs</span><span class="p">,</span>
                                     <span class="n">levels</span><span class="p">,</span> <span class="n">samples</span><span class="p">,</span> <span class="n">fignum</span><span class="p">,</span> <span class="n">ax</span><span class="p">,</span> <span class="n">resolution</span><span class="p">,</span>
                                     <span class="n">plot_raw</span><span class="o">=</span><span class="n">plot_raw</span><span class="p">,</span> <span class="n">Y_metadata</span><span class="o">=</span><span class="n">Y_metadata</span><span class="p">,</span>
                                     <span class="n">data_symbol</span><span class="o">=</span><span class="n">data_symbol</span><span class="p">,</span> <span class="o">**</span><span class="n">kw</span><span class="p">)</span>
</div>
<div class="viewcode-block" id="GP.plot"><a class="viewcode-back" href="../../../GPy.core.html#GPy.core.gp.GP.plot">[docs]</a>    <span class="k">def</span> <span class="nf">plot</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">plot_limits</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">which_data_rows</span><span class="o">=</span><span class="s">&#39;all&#39;</span><span class="p">,</span>
        <span class="n">which_data_ycols</span><span class="o">=</span><span class="s">&#39;all&#39;</span><span class="p">,</span> <span class="n">fixed_inputs</span><span class="o">=</span><span class="p">[],</span>
        <span class="n">levels</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">samples</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">fignum</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">resolution</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
        <span class="n">plot_raw</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
        <span class="n">linecol</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span><span class="n">fillcol</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">Y_metadata</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">data_symbol</span><span class="o">=</span><span class="s">&#39;kx&#39;</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Plot the posterior of the GP.</span>
<span class="sd">          - In one dimension, the function is plotted with a shaded region identifying two standard deviations.</span>
<span class="sd">          - In two dimsensions, a contour-plot shows the mean predicted function</span>
<span class="sd">          - In higher dimensions, use fixed_inputs to plot the GP  with some of the inputs fixed.</span>

<span class="sd">        Can plot only part of the data and part of the posterior functions</span>
<span class="sd">        using which_data_rowsm which_data_ycols.</span>

<span class="sd">        :param plot_limits: The limits of the plot. If 1D [xmin,xmax], if 2D [[xmin,ymin],[xmax,ymax]]. Defaluts to data limits</span>
<span class="sd">        :type plot_limits: np.array</span>
<span class="sd">        :param which_data_rows: which of the training data to plot (default all)</span>
<span class="sd">        :type which_data_rows: &#39;all&#39; or a slice object to slice model.X, model.Y</span>
<span class="sd">        :param which_data_ycols: when the data has several columns (independant outputs), only plot these</span>
<span class="sd">        :type which_data_ycols: &#39;all&#39; or a list of integers</span>
<span class="sd">        :param fixed_inputs: a list of tuple [(i,v), (i,v)...], specifying that input index i should be set to value v.</span>
<span class="sd">        :type fixed_inputs: a list of tuples</span>
<span class="sd">        :param resolution: the number of intervals to sample the GP on. Defaults to 200 in 1D and 50 (a 50x50 grid) in 2D</span>
<span class="sd">        :type resolution: int</span>
<span class="sd">        :param levels: number of levels to plot in a contour plot.</span>
<span class="sd">        :param levels: for 2D plotting, the number of contour levels to use is ax is None, create a new figure</span>
<span class="sd">        :type levels: int</span>
<span class="sd">        :param samples: the number of a posteriori samples to plot</span>
<span class="sd">        :type samples: int</span>
<span class="sd">        :param fignum: figure to plot on.</span>
<span class="sd">        :type fignum: figure number</span>
<span class="sd">        :param ax: axes to plot on.</span>
<span class="sd">        :type ax: axes handle</span>
<span class="sd">        :param linecol: color of line to plot [Tango.colorsHex[&#39;darkBlue&#39;]]</span>
<span class="sd">        :type linecol: color either as Tango.colorsHex object or character (&#39;r&#39; is red, &#39;g&#39; is green) as is standard in matplotlib</span>
<span class="sd">        :param fillcol: color of fill [Tango.colorsHex[&#39;lightBlue&#39;]]</span>
<span class="sd">        :type fillcol: color either as Tango.colorsHex object or character (&#39;r&#39; is red, &#39;g&#39; is green) as is standard in matplotlib</span>
<span class="sd">        :param Y_metadata: additional data associated with Y which may be needed</span>
<span class="sd">        :type Y_metadata: dict</span>
<span class="sd">        :param data_symbol: symbol as used matplotlib, by default this is a black cross (&#39;kx&#39;)</span>
<span class="sd">        :type data_symbol: color either as Tango.colorsHex object or character (&#39;r&#39; is red, &#39;g&#39; is green) alongside marker type, as is standard in matplotlib.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">assert</span> <span class="s">&quot;matplotlib&quot;</span> <span class="ow">in</span> <span class="n">sys</span><span class="o">.</span><span class="n">modules</span><span class="p">,</span> <span class="s">&quot;matplotlib package has not been imported.&quot;</span>
        <span class="kn">from</span> <span class="nn">..plotting.matplot_dep</span> <span class="kn">import</span> <span class="n">models_plots</span>
        <span class="n">kw</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">if</span> <span class="n">linecol</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
            <span class="n">kw</span><span class="p">[</span><span class="s">&#39;linecol&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">linecol</span>
        <span class="k">if</span> <span class="n">fillcol</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
            <span class="n">kw</span><span class="p">[</span><span class="s">&#39;fillcol&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">fillcol</span>
        <span class="k">return</span> <span class="n">models_plots</span><span class="o">.</span><span class="n">plot_fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">plot_limits</span><span class="p">,</span> <span class="n">which_data_rows</span><span class="p">,</span>
                                     <span class="n">which_data_ycols</span><span class="p">,</span> <span class="n">fixed_inputs</span><span class="p">,</span>
                                     <span class="n">levels</span><span class="p">,</span> <span class="n">samples</span><span class="p">,</span> <span class="n">fignum</span><span class="p">,</span> <span class="n">ax</span><span class="p">,</span> <span class="n">resolution</span><span class="p">,</span>
                                     <span class="n">plot_raw</span><span class="o">=</span><span class="n">plot_raw</span><span class="p">,</span> <span class="n">Y_metadata</span><span class="o">=</span><span class="n">Y_metadata</span><span class="p">,</span>
                                     <span class="n">data_symbol</span><span class="o">=</span><span class="n">data_symbol</span><span class="p">,</span> <span class="o">**</span><span class="n">kw</span><span class="p">)</span>
</div>
<div class="viewcode-block" id="GP.input_sensitivity"><a class="viewcode-back" href="../../../GPy.core.html#GPy.core.gp.GP.input_sensitivity">[docs]</a>    <span class="k">def</span> <span class="nf">input_sensitivity</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">summarize</span><span class="o">=</span><span class="bp">True</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns the sensitivity for each dimension of this model</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">kern</span><span class="o">.</span><span class="n">input_sensitivity</span><span class="p">(</span><span class="n">summarize</span><span class="o">=</span><span class="n">summarize</span><span class="p">)</span>
</div>
<div class="viewcode-block" id="GP.optimize"><a class="viewcode-back" href="../../../GPy.core.html#GPy.core.gp.GP.optimize">[docs]</a>    <span class="k">def</span> <span class="nf">optimize</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">start</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Optimize the model using self.log_likelihood and self.log_likelihood_gradient, as well as self.priors.</span>
<span class="sd">        kwargs are passed to the optimizer. They can be:</span>

<span class="sd">        :param max_f_eval: maximum number of function evaluations</span>
<span class="sd">        :type max_f_eval: int</span>
<span class="sd">        :messages: whether to display during optimisation</span>
<span class="sd">        :type messages: bool</span>
<span class="sd">        :param optimizer: which optimizer to use (defaults to self.preferred optimizer), a range of optimisers can be found in :module:`~GPy.inference.optimization`, they include &#39;scg&#39;, &#39;lbfgs&#39;, &#39;tnc&#39;.</span>
<span class="sd">        :type optimizer: string</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">inference_method</span><span class="o">.</span><span class="n">on_optimization_start</span><span class="p">()</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="nb">super</span><span class="p">(</span><span class="n">GP</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">optimize</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">start</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="k">except</span> <span class="ne">KeyboardInterrupt</span><span class="p">:</span>
            <span class="k">print</span> <span class="s">&quot;KeyboardInterrupt caught, calling on_optimization_end() to round things up&quot;</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">inference_method</span><span class="o">.</span><span class="n">on_optimization_end</span><span class="p">()</span>
            <span class="k">raise</span>
</div>
<div class="viewcode-block" id="GP.infer_newX"><a class="viewcode-back" href="../../../GPy.core.html#GPy.core.gp.GP.infer_newX">[docs]</a>    <span class="k">def</span> <span class="nf">infer_newX</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">Y_new</span><span class="p">,</span> <span class="n">optimize</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Infer the distribution of X for the new observed data *Y_new*.</span>

<span class="sd">        :param Y_new: the new observed data for inference</span>
<span class="sd">        :type Y_new: numpy.ndarray</span>
<span class="sd">        :param optimize: whether to optimize the location of new X (True by default)</span>
<span class="sd">        :type optimize: boolean</span>
<span class="sd">        :return: a tuple containing the posterior estimation of X and the model that optimize X</span>
<span class="sd">        :rtype: (:class:`~GPy.core.parameterization.variational.VariationalPosterior` or numpy.ndarray, :class:`~GPy.core.model.Model`)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="kn">from</span> <span class="nn">..inference.latent_function_inference.inferenceX</span> <span class="kn">import</span> <span class="n">infer_newX</span>
        <span class="k">return</span> <span class="n">infer_newX</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">Y_new</span><span class="p">,</span> <span class="n">optimize</span><span class="o">=</span><span class="n">optimize</span><span class="p">)</span></div></div>
</pre></div>

          </div>
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<div id="searchbox" style="display: none" role="search">
  <h3>Quick search</h3>
    <form class="search" action="../../../search.html" method="get">
      <input type="text" name="q" />
      <input type="submit" value="Go" />
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
    <p class="searchtip" style="font-size: 90%">
    Enter search terms or a module, class or function name.
    </p>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../../../genindex.html" title="General Index"
             >index</a></li>
        <li class="right" >
          <a href="../../../py-modindex.html" title="Python Module Index"
             >modules</a> |</li>
        <li class="nav-item nav-item-0"><a href="../../../index.html">GPy  documentation</a> &raquo;</li>
          <li class="nav-item nav-item-1"><a href="../../index.html" >Module code</a> &raquo;</li>
          <li class="nav-item nav-item-2"><a href="../../GPy.html" >GPy</a> &raquo;</li> 
      </ul>
    </div>
    <div class="footer" role="contentinfo">
        &copy; Copyright 2013, Author.
      Created using <a href="http://sphinx-doc.org/">Sphinx</a> 1.3.1.
    </div>
  </body>
</html>