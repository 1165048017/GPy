<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">


<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    
    <title>GPy.core.model &mdash; GPy  documentation</title>
    
    <link rel="stylesheet" href="../../../_static//default.css" type="text/css" />
    <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
    
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    '../../../',
        VERSION:     '',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true
      };
    </script>
    <script type="text/javascript" src="../../../_static/jquery.js"></script>
    <script type="text/javascript" src="../../../_static/underscore.js"></script>
    <script type="text/javascript" src="../../../_static/doctools.js"></script>
    <link rel="top" title="GPy  documentation" href="../../../index.html" />
    <link rel="up" title="GPy" href="../../GPy.html" /> 
  </head>
  <body role="document">
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../../../genindex.html" title="General Index"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="../../../py-modindex.html" title="Python Module Index"
             >modules</a> |</li>
        <li class="nav-item nav-item-0"><a href="../../../index.html">GPy  documentation</a> &raquo;</li>
          <li class="nav-item nav-item-1"><a href="../../index.html" >Module code</a> &raquo;</li>
          <li class="nav-item nav-item-2"><a href="../../GPy.html" accesskey="U">GPy</a> &raquo;</li> 
      </ul>
    </div>  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <h1>Source code for GPy.core.model</h1><div class="highlight"><pre>
<span class="c"># Copyright (c) 2012-2014, GPy authors (see AUTHORS.txt).</span>
<span class="c"># Licensed under the BSD 3-clause license (see LICENSE.txt)</span>


<span class="kn">from</span> <span class="nn">..</span> <span class="kn">import</span> <span class="n">likelihoods</span>
<span class="kn">from</span> <span class="nn">..inference</span> <span class="kn">import</span> <span class="n">optimization</span>
<span class="kn">from</span> <span class="nn">..util.misc</span> <span class="kn">import</span> <span class="n">opt_wrapper</span>
<span class="kn">from</span> <span class="nn">parameterization</span> <span class="kn">import</span> <span class="n">Parameterized</span>
<span class="kn">import</span> <span class="nn">multiprocessing</span> <span class="kn">as</span> <span class="nn">mp</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">numpy.linalg.linalg</span> <span class="kn">import</span> <span class="n">LinAlgError</span>
<span class="kn">import</span> <span class="nn">itertools</span>
<span class="c"># import numdifftools as ndt</span>

<div class="viewcode-block" id="Model"><a class="viewcode-back" href="../../../GPy.core.html#GPy.core.model.Model">[docs]</a><span class="k">class</span> <span class="nc">Model</span><span class="p">(</span><span class="n">Parameterized</span><span class="p">):</span>
    <span class="n">_fail_count</span> <span class="o">=</span> <span class="mi">0</span>  <span class="c"># Count of failed optimization steps (see objective)</span>
    <span class="n">_allowed_failures</span> <span class="o">=</span> <span class="mi">10</span>  <span class="c"># number of allowed failures</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Model</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">__init__</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>  <span class="c"># Parameterized.__init__(self)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">optimization_runs</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sampling_runs</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">preferred_optimizer</span> <span class="o">=</span> <span class="s">&#39;bfgs&#39;</span>
        <span class="kn">from</span> <span class="nn">.parameterization.ties_and_remappings</span> <span class="kn">import</span> <span class="n">Tie</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tie</span> <span class="o">=</span> <span class="n">Tie</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">link_parameter</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">tie</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_observer</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">tie</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">tie</span><span class="o">.</span><span class="n">_parameters_changed_notification</span><span class="p">,</span> <span class="n">priority</span><span class="o">=-</span><span class="mi">500</span><span class="p">)</span>

<div class="viewcode-block" id="Model.log_likelihood"><a class="viewcode-back" href="../../../GPy.core.html#GPy.core.model.Model.log_likelihood">[docs]</a>    <span class="k">def</span> <span class="nf">log_likelihood</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">,</span> <span class="s">&quot;this needs to be implemented to use the model class&quot;</span></div>
    <span class="k">def</span> <span class="nf">_log_likelihood_gradients</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">gradient</span>

<div class="viewcode-block" id="Model.optimize_restarts"><a class="viewcode-back" href="../../../GPy.core.html#GPy.core.model.Model.optimize_restarts">[docs]</a>    <span class="k">def</span> <span class="nf">optimize_restarts</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_restarts</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">robust</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">parallel</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">num_processes</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Perform random restarts of the model, and set the model to the best</span>
<span class="sd">        seen solution.</span>

<span class="sd">        If the robust flag is set, exceptions raised during optimizations will</span>
<span class="sd">        be handled silently.  If _all_ runs fail, the model is reset to the</span>
<span class="sd">        existing parameter values.</span>

<span class="sd">        **Notes**</span>

<span class="sd">        :param num_restarts: number of restarts to use (default 10)</span>
<span class="sd">        :type num_restarts: int</span>
<span class="sd">        :param robust: whether to handle exceptions silently or not (default False)</span>
<span class="sd">        :type robust: bool</span>
<span class="sd">        :param parallel: whether to run each restart as a separate process. It relies on the multiprocessing module.</span>
<span class="sd">        :type parallel: bool</span>
<span class="sd">        :param num_processes: number of workers in the multiprocessing pool</span>
<span class="sd">        :type numprocesses: int</span>

<span class="sd">        \*\*kwargs are passed to the optimizer. They can be:</span>

<span class="sd">        :param max_f_eval: maximum number of function evaluations</span>
<span class="sd">        :type max_f_eval: int</span>
<span class="sd">        :param max_iters: maximum number of iterations</span>
<span class="sd">        :type max_iters: int</span>
<span class="sd">        :param messages: whether to display during optimisation</span>
<span class="sd">        :type messages: bool</span>

<span class="sd">        .. note:: If num_processes is None, the number of workes in the</span>
<span class="sd">        multiprocessing pool is automatically set to the number of processors</span>
<span class="sd">        on the current machine.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">initial_parameters</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizer_array</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>

        <span class="k">if</span> <span class="n">parallel</span><span class="p">:</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="n">jobs</span> <span class="o">=</span> <span class="p">[]</span>
                <span class="n">pool</span> <span class="o">=</span> <span class="n">mp</span><span class="o">.</span><span class="n">Pool</span><span class="p">(</span><span class="n">processes</span><span class="o">=</span><span class="n">num_processes</span><span class="p">)</span>
                <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_restarts</span><span class="p">):</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">randomize</span><span class="p">()</span>
                    <span class="n">job</span> <span class="o">=</span> <span class="n">pool</span><span class="o">.</span><span class="n">apply_async</span><span class="p">(</span><span class="n">opt_wrapper</span><span class="p">,</span> <span class="n">args</span><span class="o">=</span><span class="p">(</span><span class="bp">self</span><span class="p">,),</span> <span class="n">kwds</span><span class="o">=</span><span class="n">kwargs</span><span class="p">)</span>
                    <span class="n">jobs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">job</span><span class="p">)</span>

                <span class="n">pool</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>  <span class="c"># signal that no more data coming in</span>
                <span class="n">pool</span><span class="o">.</span><span class="n">join</span><span class="p">()</span>  <span class="c"># wait for all the tasks to complete</span>
            <span class="k">except</span> <span class="ne">KeyboardInterrupt</span><span class="p">:</span>
                <span class="k">print</span> <span class="s">&quot;Ctrl+c received, terminating and joining pool.&quot;</span>
                <span class="n">pool</span><span class="o">.</span><span class="n">terminate</span><span class="p">()</span>
                <span class="n">pool</span><span class="o">.</span><span class="n">join</span><span class="p">()</span>

        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_restarts</span><span class="p">):</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="n">parallel</span><span class="p">:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">randomize</span><span class="p">()</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">optimize</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">optimization_runs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">jobs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">get</span><span class="p">())</span>

                <span class="k">if</span> <span class="n">verbose</span><span class="p">:</span>
                    <span class="k">print</span><span class="p">(</span><span class="s">&quot;Optimization restart {0}/{1}, f = {2}&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">num_restarts</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimization_runs</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">f_opt</span><span class="p">))</span>
            <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">robust</span><span class="p">:</span>
                    <span class="k">print</span><span class="p">(</span><span class="s">&quot;Warning - optimization restart {0}/{1} failed&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">num_restarts</span><span class="p">))</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="n">e</span>

        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">optimization_runs</span><span class="p">):</span>
            <span class="n">i</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmin</span><span class="p">([</span><span class="n">o</span><span class="o">.</span><span class="n">f_opt</span> <span class="k">for</span> <span class="n">o</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimization_runs</span><span class="p">])</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">optimizer_array</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimization_runs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">x_opt</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">optimizer_array</span> <span class="o">=</span> <span class="n">initial_parameters</span>
</div>
<div class="viewcode-block" id="Model.ensure_default_constraints"><a class="viewcode-back" href="../../../GPy.core.html#GPy.core.model.Model.ensure_default_constraints">[docs]</a>    <span class="k">def</span> <span class="nf">ensure_default_constraints</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">warning</span><span class="o">=</span><span class="bp">True</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Ensure that any variables which should clearly be positive</span>
<span class="sd">        have been constrained somehow. The method performs a regular</span>
<span class="sd">        expression search on parameter names looking for the terms</span>
<span class="sd">        &#39;variance&#39;, &#39;lengthscale&#39;, &#39;precision&#39; and &#39;kappa&#39;. If any of</span>
<span class="sd">        these terms are present in the name the parameter is</span>
<span class="sd">        constrained positive.</span>

<span class="sd">        DEPRECATED.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">raise</span> <span class="ne">DeprecationWarning</span><span class="p">,</span> <span class="s">&#39;parameters now have default constraints&#39;</span>
</div>
<div class="viewcode-block" id="Model.objective_function"><a class="viewcode-back" href="../../../GPy.core.html#GPy.core.model.Model.objective_function">[docs]</a>    <span class="k">def</span> <span class="nf">objective_function</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        The objective function for the given algorithm.</span>

<span class="sd">        This function is the true objective, which wants to be minimized.</span>
<span class="sd">        Note that all parameters are already set and in place, so you just need</span>
<span class="sd">        to return the objective function here.</span>

<span class="sd">        For probabilistic models this is the negative log_likelihood</span>
<span class="sd">        (including the MAP prior), so we return it here. If your model is not</span>
<span class="sd">        probabilistic, just return your objective to minimize here!</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="o">-</span><span class="nb">float</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">log_likelihood</span><span class="p">())</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">log_prior</span><span class="p">()</span>
</div>
<div class="viewcode-block" id="Model.objective_function_gradients"><a class="viewcode-back" href="../../../GPy.core.html#GPy.core.model.Model.objective_function_gradients">[docs]</a>    <span class="k">def</span> <span class="nf">objective_function_gradients</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        The gradients for the objective function for the given algorithm.</span>
<span class="sd">        The gradients are w.r.t. the *negative* objective function, as</span>
<span class="sd">        this framework works with *negative* log-likelihoods as a default.</span>

<span class="sd">        You can find the gradient for the parameters in self.gradient at all times.</span>
<span class="sd">        This is the place, where gradients get stored for parameters.</span>

<span class="sd">        This function is the true objective, which wants to be minimized.</span>
<span class="sd">        Note that all parameters are already set and in place, so you just need</span>
<span class="sd">        to return the gradient here.</span>

<span class="sd">        For probabilistic models this is the gradient of the negative log_likelihood</span>
<span class="sd">        (including the MAP prior), so we return it here. If your model is not</span>
<span class="sd">        probabilistic, just return your *negative* gradient here!</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="o">-</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_log_likelihood_gradients</span><span class="p">()</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">_log_prior_gradients</span><span class="p">())</span>
</div>
    <span class="k">def</span> <span class="nf">_grads</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Gets the gradients from the likelihood and the priors.</span>

<span class="sd">        Failures are handled robustly. The algorithm will try several times to</span>
<span class="sd">        return the gradients, and will raise the original exception if</span>
<span class="sd">        the objective cannot be computed.</span>

<span class="sd">        :param x: the parameters of the model.</span>
<span class="sd">        :type x: np.array</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="c"># self._set_params_transformed(x)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">optimizer_array</span> <span class="o">=</span> <span class="n">x</span>
            <span class="n">obj_grads</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_transform_gradients</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">objective_function_gradients</span><span class="p">())</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_fail_count</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">except</span> <span class="p">(</span><span class="n">LinAlgError</span><span class="p">,</span> <span class="ne">ZeroDivisionError</span><span class="p">,</span> <span class="ne">ValueError</span><span class="p">):</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_fail_count</span> <span class="o">&gt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_allowed_failures</span><span class="p">:</span>
                <span class="k">raise</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_fail_count</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="n">obj_grads</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">clip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_transform_gradients</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">objective_function_gradients</span><span class="p">()),</span> <span class="o">-</span><span class="mf">1e100</span><span class="p">,</span> <span class="mf">1e100</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">obj_grads</span>

    <span class="k">def</span> <span class="nf">_objective</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        The objective function passed to the optimizer. It combines</span>
<span class="sd">        the likelihood and the priors.</span>

<span class="sd">        Failures are handled robustly. The algorithm will try several times to</span>
<span class="sd">        return the objective, and will raise the original exception if</span>
<span class="sd">        the objective cannot be computed.</span>

<span class="sd">        :param x: the parameters of the model.</span>
<span class="sd">        :parameter type: np.array</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">optimizer_array</span> <span class="o">=</span> <span class="n">x</span>
            <span class="n">obj</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">objective_function</span><span class="p">()</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_fail_count</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">except</span> <span class="p">(</span><span class="n">LinAlgError</span><span class="p">,</span> <span class="ne">ZeroDivisionError</span><span class="p">,</span> <span class="ne">ValueError</span><span class="p">):</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_fail_count</span> <span class="o">&gt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_allowed_failures</span><span class="p">:</span>
                <span class="k">raise</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_fail_count</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">inf</span>
        <span class="k">return</span> <span class="n">obj</span>

    <span class="k">def</span> <span class="nf">_objective_grads</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">optimizer_array</span> <span class="o">=</span> <span class="n">x</span>
            <span class="n">obj_f</span><span class="p">,</span> <span class="n">obj_grads</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">objective_function</span><span class="p">(),</span> <span class="bp">self</span><span class="o">.</span><span class="n">_transform_gradients</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">objective_function_gradients</span><span class="p">())</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_fail_count</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">except</span> <span class="p">(</span><span class="n">LinAlgError</span><span class="p">,</span> <span class="ne">ZeroDivisionError</span><span class="p">,</span> <span class="ne">ValueError</span><span class="p">):</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_fail_count</span> <span class="o">&gt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_allowed_failures</span><span class="p">:</span>
                <span class="k">raise</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_fail_count</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="n">obj_f</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">inf</span>
            <span class="n">obj_grads</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">clip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_transform_gradients</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">objective_function_gradients</span><span class="p">()),</span> <span class="o">-</span><span class="mf">1e100</span><span class="p">,</span> <span class="mf">1e100</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">obj_f</span><span class="p">,</span> <span class="n">obj_grads</span>

<div class="viewcode-block" id="Model.optimize"><a class="viewcode-back" href="../../../GPy.core.html#GPy.core.model.Model.optimize">[docs]</a>    <span class="k">def</span> <span class="nf">optimize</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">start</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Optimize the model using self.log_likelihood and self.log_likelihood_gradient, as well as self.priors.</span>

<span class="sd">        kwargs are passed to the optimizer. They can be:</span>

<span class="sd">        :param max_f_eval: maximum number of function evaluations</span>
<span class="sd">        :type max_f_eval: int</span>
<span class="sd">        :messages: whether to display during optimisation</span>
<span class="sd">        :type messages: bool</span>
<span class="sd">        :param optimizer: which optimizer to use (defaults to self.preferred optimizer)</span>
<span class="sd">        :type optimizer: string</span>

<span class="sd">        Valid optimizers are:</span>
<span class="sd">          - &#39;scg&#39;: scaled conjugate gradient method, recommended for stability.</span>
<span class="sd">                   See also GPy.inference.optimization.scg</span>
<span class="sd">          - &#39;fmin_tnc&#39;: truncated Newton method (see scipy.optimize.fmin_tnc)</span>
<span class="sd">          - &#39;simplex&#39;: the Nelder-Mead simplex method (see scipy.optimize.fmin),</span>
<span class="sd">          - &#39;lbfgsb&#39;: the l-bfgs-b method (see scipy.optimize.fmin_l_bfgs_b),</span>
<span class="sd">          - &#39;sgd&#39;: stochastic gradient decsent (see scipy.optimize.sgd). For experts only!</span>


<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_fixed</span><span class="p">:</span>
            <span class="k">print</span> <span class="s">&#39;nothing to optimize&#39;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">size</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">print</span> <span class="s">&#39;nothing to optimize&#39;</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">update_model</span><span class="p">():</span>
            <span class="k">print</span> <span class="s">&quot;setting updates on again&quot;</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">update_model</span><span class="p">(</span><span class="bp">True</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">start</span> <span class="o">==</span> <span class="bp">None</span><span class="p">:</span>
            <span class="n">start</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizer_array</span>

        <span class="k">if</span> <span class="n">optimizer</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
            <span class="n">optimizer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">preferred_optimizer</span>

        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">optimization</span><span class="o">.</span><span class="n">Optimizer</span><span class="p">):</span>
            <span class="n">opt</span> <span class="o">=</span> <span class="n">optimizer</span>
            <span class="n">opt</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="bp">self</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">optimizer</span> <span class="o">=</span> <span class="n">optimization</span><span class="o">.</span><span class="n">get_optimizer</span><span class="p">(</span><span class="n">optimizer</span><span class="p">)</span>
            <span class="n">opt</span> <span class="o">=</span> <span class="n">optimizer</span><span class="p">(</span><span class="n">start</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="bp">self</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

        <span class="n">opt</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">f_fp</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_objective_grads</span><span class="p">,</span> <span class="n">f</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_objective</span><span class="p">,</span> <span class="n">fp</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_grads</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">optimization_runs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">opt</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">optimizer_array</span> <span class="o">=</span> <span class="n">opt</span><span class="o">.</span><span class="n">x_opt</span>
</div>
<div class="viewcode-block" id="Model.optimize_SGD"><a class="viewcode-back" href="../../../GPy.core.html#GPy.core.model.Model.optimize_SGD">[docs]</a>    <span class="k">def</span> <span class="nf">optimize_SGD</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">iterations</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="c"># assert self.Y.shape[1] &gt; 1, &quot;SGD only works with D &gt; 1&quot;</span>
        <span class="n">sgd</span> <span class="o">=</span> <span class="n">SGD</span><span class="o">.</span><span class="n">StochasticGD</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">iterations</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">,</span> <span class="n">momentum</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>  <span class="c"># @UndefinedVariable</span>
        <span class="n">sgd</span><span class="o">.</span><span class="n">run</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">optimization_runs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">sgd</span><span class="p">)</span>
</div>
    <span class="k">def</span> <span class="nf">_checkgrad</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">target_param</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">,</span> <span class="n">tolerance</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">,</span> <span class="n">df_tolerance</span><span class="o">=</span><span class="mf">1e-12</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Check the gradient of the ,odel by comparing to a numerical</span>
<span class="sd">        estimate.  If the verbose flag is passed, individual</span>
<span class="sd">        components are tested (and printed)</span>

<span class="sd">        :param verbose: If True, print a &quot;full&quot; checking of each parameter</span>
<span class="sd">        :type verbose: bool</span>
<span class="sd">        :param step: The size of the step around which to linearise the objective</span>
<span class="sd">        :type step: float (default 1e-6)</span>
<span class="sd">        :param tolerance: the tolerance allowed (see note)</span>
<span class="sd">        :type tolerance: float (default 1e-3)</span>

<span class="sd">        Note:-</span>
<span class="sd">           The gradient is considered correct if the ratio of the analytical</span>
<span class="sd">           and numerical gradients is within &lt;tolerance&gt; of unity.</span>

<span class="sd">           The *dF_ratio* indicates the limit of numerical accuracy of numerical gradients.</span>
<span class="sd">           If it is too small, e.g., smaller than 1e-12, the numerical gradients are usually</span>
<span class="sd">           not accurate enough for the tests (shown with blue).</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizer_array</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="n">verbose</span><span class="p">:</span>
            <span class="c"># make sure only to test the selected parameters</span>
            <span class="k">if</span> <span class="n">target_param</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
                <span class="n">transformed_index</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">transformed_index</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_raveled_index_for</span><span class="p">(</span><span class="n">target_param</span><span class="p">)</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_has_fixes</span><span class="p">():</span>
                    <span class="n">indices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">r_</span><span class="p">[:</span><span class="bp">self</span><span class="o">.</span><span class="n">size</span><span class="p">]</span>
                    <span class="n">which</span> <span class="o">=</span> <span class="p">(</span><span class="n">transformed_index</span><span class="p">[:,</span> <span class="bp">None</span><span class="p">]</span> <span class="o">==</span> <span class="n">indices</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_fixes_</span><span class="p">][</span><span class="bp">None</span><span class="p">,</span> <span class="p">:])</span><span class="o">.</span><span class="n">nonzero</span><span class="p">()</span>
                    <span class="n">transformed_index</span> <span class="o">=</span> <span class="p">(</span><span class="n">indices</span> <span class="o">-</span> <span class="p">(</span><span class="o">~</span><span class="bp">self</span><span class="o">.</span><span class="n">_fixes_</span><span class="p">)</span><span class="o">.</span><span class="n">cumsum</span><span class="p">())[</span><span class="n">transformed_index</span><span class="p">[</span><span class="n">which</span><span class="p">[</span><span class="mi">0</span><span class="p">]]]</span>

                <span class="k">if</span> <span class="n">transformed_index</span><span class="o">.</span><span class="n">size</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="k">print</span> <span class="s">&quot;No free parameters to check&quot;</span>
                    <span class="k">return</span>

            <span class="c"># just check the global ratio</span>
            <span class="n">dx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
            <span class="n">dx</span><span class="p">[</span><span class="n">transformed_index</span><span class="p">]</span> <span class="o">=</span> <span class="n">step</span> <span class="o">*</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sign</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">transformed_index</span><span class="o">.</span><span class="n">size</span><span class="p">))</span> <span class="k">if</span> <span class="n">transformed_index</span><span class="o">.</span><span class="n">size</span> <span class="o">!=</span> <span class="mi">2</span> <span class="k">else</span> <span class="mf">1.</span><span class="p">)</span>

            <span class="c"># evaulate around the point x</span>
            <span class="n">f1</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_objective</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="n">dx</span><span class="p">)</span>
            <span class="n">f2</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_objective</span><span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">dx</span><span class="p">)</span>
            <span class="n">gradient</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_grads</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

            <span class="n">dx</span> <span class="o">=</span> <span class="n">dx</span><span class="p">[</span><span class="n">transformed_index</span><span class="p">]</span>
            <span class="n">gradient</span> <span class="o">=</span> <span class="n">gradient</span><span class="p">[</span><span class="n">transformed_index</span><span class="p">]</span>

            <span class="n">denominator</span> <span class="o">=</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">dx</span><span class="p">,</span> <span class="n">gradient</span><span class="p">))</span>
            <span class="n">global_ratio</span> <span class="o">=</span> <span class="p">(</span><span class="n">f1</span> <span class="o">-</span> <span class="n">f2</span><span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">denominator</span> <span class="o">==</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">1e-32</span><span class="p">,</span> <span class="n">denominator</span><span class="p">)</span>
            <span class="n">global_diff</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">f1</span> <span class="o">-</span> <span class="n">f2</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">tolerance</span> <span class="ow">and</span> <span class="n">np</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">gradient</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">atol</span><span class="o">=</span><span class="n">tolerance</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">global_ratio</span> <span class="ow">is</span> <span class="n">np</span><span class="o">.</span><span class="n">nan</span><span class="p">:</span>
                <span class="n">global_ratio</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="mf">1.</span> <span class="o">-</span> <span class="n">global_ratio</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">tolerance</span> <span class="ow">or</span> <span class="n">global_diff</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c"># check the gradient of each parameter individually, and do some pretty printing</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="n">names</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_param_names</span><span class="p">()</span>
            <span class="k">except</span> <span class="ne">NotImplementedError</span><span class="p">:</span>
                <span class="n">names</span> <span class="o">=</span> <span class="p">[</span><span class="s">&#39;Variable </span><span class="si">%i</span><span class="s">&#39;</span> <span class="o">%</span> <span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">))]</span>
            <span class="c"># Prepare for pretty-printing</span>
            <span class="n">header</span> <span class="o">=</span> <span class="p">[</span><span class="s">&#39;Name&#39;</span><span class="p">,</span> <span class="s">&#39;Ratio&#39;</span><span class="p">,</span> <span class="s">&#39;Difference&#39;</span><span class="p">,</span> <span class="s">&#39;Analytical&#39;</span><span class="p">,</span> <span class="s">&#39;Numerical&#39;</span><span class="p">,</span> <span class="s">&#39;dF_ratio&#39;</span><span class="p">]</span>
            <span class="n">max_names</span> <span class="o">=</span> <span class="nb">max</span><span class="p">([</span><span class="nb">len</span><span class="p">(</span><span class="n">names</span><span class="p">[</span><span class="n">i</span><span class="p">])</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">names</span><span class="p">))]</span> <span class="o">+</span> <span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="n">header</span><span class="p">[</span><span class="mi">0</span><span class="p">])])</span>
            <span class="n">float_len</span> <span class="o">=</span> <span class="mi">10</span>
            <span class="n">cols</span> <span class="o">=</span> <span class="p">[</span><span class="n">max_names</span><span class="p">]</span>
            <span class="n">cols</span><span class="o">.</span><span class="n">extend</span><span class="p">([</span><span class="nb">max</span><span class="p">(</span><span class="n">float_len</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">header</span><span class="p">[</span><span class="n">i</span><span class="p">]))</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">header</span><span class="p">))])</span>
            <span class="n">cols</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">cols</span><span class="p">)</span> <span class="o">+</span> <span class="mi">5</span>
            <span class="n">header_string</span> <span class="o">=</span> <span class="p">[</span><span class="s">&quot;{h:^{col}}&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">h</span><span class="o">=</span><span class="n">header</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">col</span><span class="o">=</span><span class="n">cols</span><span class="p">[</span><span class="n">i</span><span class="p">])</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">cols</span><span class="p">))]</span>
            <span class="n">header_string</span> <span class="o">=</span> <span class="nb">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="s">&#39;|&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="p">[</span><span class="n">header_string</span><span class="p">])</span>
            <span class="n">separator</span> <span class="o">=</span> <span class="s">&#39;-&#39;</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">header_string</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
            <span class="k">print</span> <span class="s">&#39;</span><span class="se">\n</span><span class="s">&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">([</span><span class="n">header_string</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">separator</span><span class="p">])</span>
            <span class="k">if</span> <span class="n">target_param</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
                <span class="n">param_index</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
                <span class="n">transformed_index</span> <span class="o">=</span> <span class="n">param_index</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">param_index</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_raveled_index_for</span><span class="p">(</span><span class="n">target_param</span><span class="p">)</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_has_fixes</span><span class="p">():</span>
                    <span class="n">indices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">r_</span><span class="p">[:</span><span class="bp">self</span><span class="o">.</span><span class="n">size</span><span class="p">]</span>
                    <span class="n">which</span> <span class="o">=</span> <span class="p">(</span><span class="n">param_index</span><span class="p">[:,</span> <span class="bp">None</span><span class="p">]</span> <span class="o">==</span> <span class="n">indices</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_fixes_</span><span class="p">][</span><span class="bp">None</span><span class="p">,</span> <span class="p">:])</span><span class="o">.</span><span class="n">nonzero</span><span class="p">()</span>
                    <span class="n">param_index</span> <span class="o">=</span> <span class="n">param_index</span><span class="p">[</span><span class="n">which</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span>
                    <span class="n">transformed_index</span> <span class="o">=</span> <span class="p">(</span><span class="n">indices</span> <span class="o">-</span> <span class="p">(</span><span class="o">~</span><span class="bp">self</span><span class="o">.</span><span class="n">_fixes_</span><span class="p">)</span><span class="o">.</span><span class="n">cumsum</span><span class="p">())[</span><span class="n">param_index</span><span class="p">]</span>
                    <span class="c"># print param_index, transformed_index</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">transformed_index</span> <span class="o">=</span> <span class="n">param_index</span>

                <span class="k">if</span> <span class="n">param_index</span><span class="o">.</span><span class="n">size</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="k">print</span> <span class="s">&quot;No free parameters to check&quot;</span>
                    <span class="k">return</span>

            <span class="n">gradient</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_grads</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
            <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">gradient</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="mf">1e-312</span><span class="p">,</span> <span class="n">gradient</span><span class="p">)</span>
            <span class="n">ret</span> <span class="o">=</span> <span class="bp">True</span>
            <span class="k">for</span> <span class="n">nind</span><span class="p">,</span> <span class="n">xind</span> <span class="ow">in</span> <span class="n">itertools</span><span class="o">.</span><span class="n">izip</span><span class="p">(</span><span class="n">param_index</span><span class="p">,</span> <span class="n">transformed_index</span><span class="p">):</span>
                <span class="n">xx</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
                <span class="n">xx</span><span class="p">[</span><span class="n">xind</span><span class="p">]</span> <span class="o">+=</span> <span class="n">step</span>
                <span class="n">f1</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_objective</span><span class="p">(</span><span class="n">xx</span><span class="p">)</span>
                <span class="n">xx</span><span class="p">[</span><span class="n">xind</span><span class="p">]</span> <span class="o">-=</span> <span class="mf">2.</span><span class="o">*</span><span class="n">step</span>
                <span class="n">f2</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_objective</span><span class="p">(</span><span class="n">xx</span><span class="p">)</span>
                <span class="n">df_ratio</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">((</span><span class="n">f1</span><span class="o">-</span><span class="n">f2</span><span class="p">)</span><span class="o">/</span><span class="nb">min</span><span class="p">(</span><span class="n">f1</span><span class="p">,</span><span class="n">f2</span><span class="p">))</span>
                <span class="n">df_unstable</span> <span class="o">=</span> <span class="n">df_ratio</span><span class="o">&lt;</span><span class="n">df_tolerance</span>
                <span class="n">numerical_gradient</span> <span class="o">=</span> <span class="p">(</span><span class="n">f1</span> <span class="o">-</span> <span class="n">f2</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">step</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">gradient</span><span class="p">[</span><span class="n">xind</span><span class="p">]</span> <span class="o">==</span> <span class="mi">0</span><span class="p">):</span> <span class="n">ratio</span> <span class="o">=</span> <span class="p">(</span><span class="n">f1</span> <span class="o">-</span> <span class="n">f2</span><span class="p">)</span> <span class="o">==</span> <span class="n">gradient</span><span class="p">[</span><span class="n">xind</span><span class="p">]</span>
                <span class="k">else</span><span class="p">:</span> <span class="n">ratio</span> <span class="o">=</span> <span class="p">(</span><span class="n">f1</span> <span class="o">-</span> <span class="n">f2</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">step</span> <span class="o">*</span> <span class="n">gradient</span><span class="p">[</span><span class="n">xind</span><span class="p">])</span>
                <span class="n">difference</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">numerical_gradient</span> <span class="o">-</span> <span class="n">gradient</span><span class="p">[</span><span class="n">xind</span><span class="p">])</span>

                <span class="k">if</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="mf">1.</span> <span class="o">-</span> <span class="n">ratio</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">tolerance</span><span class="p">)</span> <span class="ow">or</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">difference</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">tolerance</span><span class="p">:</span>
                    <span class="n">formatted_name</span> <span class="o">=</span> <span class="s">&quot;</span><span class="se">\033</span><span class="s">[92m {0} </span><span class="se">\033</span><span class="s">[0m&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">names</span><span class="p">[</span><span class="n">nind</span><span class="p">])</span>
                    <span class="n">ret</span> <span class="o">&amp;=</span> <span class="bp">True</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">formatted_name</span> <span class="o">=</span> <span class="s">&quot;</span><span class="se">\033</span><span class="s">[91m {0} </span><span class="se">\033</span><span class="s">[0m&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">names</span><span class="p">[</span><span class="n">nind</span><span class="p">])</span>
                    <span class="n">ret</span> <span class="o">&amp;=</span> <span class="bp">False</span>
                <span class="k">if</span> <span class="n">df_unstable</span><span class="p">:</span>
                    <span class="n">formatted_name</span> <span class="o">=</span> <span class="s">&quot;</span><span class="se">\033</span><span class="s">[94m {0} </span><span class="se">\033</span><span class="s">[0m&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">names</span><span class="p">[</span><span class="n">nind</span><span class="p">])</span>

                <span class="n">r</span> <span class="o">=</span> <span class="s">&#39;</span><span class="si">%.6f</span><span class="s">&#39;</span> <span class="o">%</span> <span class="nb">float</span><span class="p">(</span><span class="n">ratio</span><span class="p">)</span>
                <span class="n">d</span> <span class="o">=</span> <span class="s">&#39;</span><span class="si">%.6f</span><span class="s">&#39;</span> <span class="o">%</span> <span class="nb">float</span><span class="p">(</span><span class="n">difference</span><span class="p">)</span>
                <span class="n">g</span> <span class="o">=</span> <span class="s">&#39;</span><span class="si">%.6f</span><span class="s">&#39;</span> <span class="o">%</span> <span class="n">gradient</span><span class="p">[</span><span class="n">xind</span><span class="p">]</span>
                <span class="n">ng</span> <span class="o">=</span> <span class="s">&#39;</span><span class="si">%.6f</span><span class="s">&#39;</span> <span class="o">%</span> <span class="nb">float</span><span class="p">(</span><span class="n">numerical_gradient</span><span class="p">)</span>
                <span class="n">df</span> <span class="o">=</span> <span class="s">&#39;%1.e&#39;</span> <span class="o">%</span> <span class="nb">float</span><span class="p">(</span><span class="n">df_ratio</span><span class="p">)</span>
                <span class="n">grad_string</span> <span class="o">=</span> <span class="s">&quot;{0:&lt;{c0}}|{1:^{c1}}|{2:^{c2}}|{3:^{c3}}|{4:^{c4}}|{5:^{c5}}&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">formatted_name</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="n">g</span><span class="p">,</span> <span class="n">ng</span><span class="p">,</span> <span class="n">df</span><span class="p">,</span> <span class="n">c0</span><span class="o">=</span><span class="n">cols</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="mi">9</span><span class="p">,</span> <span class="n">c1</span><span class="o">=</span><span class="n">cols</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">c2</span><span class="o">=</span><span class="n">cols</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="n">c3</span><span class="o">=</span><span class="n">cols</span><span class="p">[</span><span class="mi">3</span><span class="p">],</span> <span class="n">c4</span><span class="o">=</span><span class="n">cols</span><span class="p">[</span><span class="mi">4</span><span class="p">],</span> <span class="n">c5</span><span class="o">=</span><span class="n">cols</span><span class="p">[</span><span class="mi">5</span><span class="p">])</span>
                <span class="k">print</span> <span class="n">grad_string</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">optimizer_array</span> <span class="o">=</span> <span class="n">x</span>
            <span class="k">return</span> <span class="n">ret</span>

    <span class="k">def</span> <span class="nf">_repr_html_</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Representation of the model in html for notebook display.&quot;&quot;&quot;</span>
        <span class="n">model_details</span> <span class="o">=</span> <span class="p">[[</span><span class="s">&#39;&lt;b&gt;Model&lt;/b&gt;&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span> <span class="o">+</span> <span class="s">&#39;&lt;br&gt;&#39;</span><span class="p">],</span>
                         <span class="p">[</span><span class="s">&#39;&lt;b&gt;Log-likelihood&lt;/b&gt;&#39;</span><span class="p">,</span> <span class="s">&#39;{}&lt;br&gt;&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">float</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">log_likelihood</span><span class="p">()))],</span>
                         <span class="p">[</span><span class="s">&quot;&lt;b&gt;Number of Parameters&lt;/b&gt;&quot;</span><span class="p">,</span> <span class="s">&#39;{}&lt;br&gt;&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">size</span><span class="p">)]]</span>
        <span class="kn">from</span> <span class="nn">operator</span> <span class="kn">import</span> <span class="n">itemgetter</span>
        <span class="n">to_print</span> <span class="o">=</span> <span class="p">[</span><span class="s">&quot;&quot;</span><span class="p">]</span> <span class="o">+</span> <span class="p">[</span><span class="s">&quot;{}: {}&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">detail</span><span class="p">)</span> <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">detail</span> <span class="ow">in</span> <span class="n">model_details</span><span class="p">]</span> <span class="o">+</span> <span class="p">[</span><span class="s">&quot;&lt;br&gt;&lt;b&gt;Parameters&lt;/b&gt;:&quot;</span><span class="p">]</span>
        <span class="n">to_print</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="nb">super</span><span class="p">(</span><span class="n">Model</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">_repr_html_</span><span class="p">())</span>
        <span class="k">return</span> <span class="s">&quot;</span><span class="se">\n</span><span class="s">&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">to_print</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">__str__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">model_details</span> <span class="o">=</span> <span class="p">[[</span><span class="s">&#39;Name&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">],</span>
                         <span class="p">[</span><span class="s">&#39;Log-likelihood&#39;</span><span class="p">,</span> <span class="s">&#39;{}&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">float</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">log_likelihood</span><span class="p">()))],</span>
                         <span class="p">[</span><span class="s">&quot;Number of Parameters&quot;</span><span class="p">,</span> <span class="s">&#39;{}&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">size</span><span class="p">)]]</span>
        <span class="kn">from</span> <span class="nn">operator</span> <span class="kn">import</span> <span class="n">itemgetter</span>
        <span class="n">max_len</span> <span class="o">=</span> <span class="nb">reduce</span><span class="p">(</span><span class="k">lambda</span> <span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">:</span> <span class="nb">max</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">b</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span> <span class="n">a</span><span class="p">),</span> <span class="n">model_details</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
        <span class="n">to_print</span> <span class="o">=</span> <span class="p">[</span><span class="s">&quot;&quot;</span><span class="p">]</span> <span class="o">+</span> <span class="p">[</span><span class="s">&quot;{0:{l}} : {1}&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">detail</span><span class="p">,</span> <span class="n">l</span><span class="o">=</span><span class="n">max_len</span><span class="p">)</span> <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">detail</span> <span class="ow">in</span> <span class="n">model_details</span><span class="p">]</span> <span class="o">+</span> <span class="p">[</span><span class="s">&quot;Parameters:&quot;</span><span class="p">]</span>
        <span class="n">to_print</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="nb">super</span><span class="p">(</span><span class="n">Model</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">__str__</span><span class="p">())</span>
        <span class="k">return</span> <span class="s">&quot;</span><span class="se">\n</span><span class="s">&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">to_print</span><span class="p">)</span>
</pre></div></div>

          </div>
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<div id="searchbox" style="display: none" role="search">
  <h3>Quick search</h3>
    <form class="search" action="../../../search.html" method="get">
      <input type="text" name="q" />
      <input type="submit" value="Go" />
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
    <p class="searchtip" style="font-size: 90%">
    Enter search terms or a module, class or function name.
    </p>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../../../genindex.html" title="General Index"
             >index</a></li>
        <li class="right" >
          <a href="../../../py-modindex.html" title="Python Module Index"
             >modules</a> |</li>
        <li class="nav-item nav-item-0"><a href="../../../index.html">GPy  documentation</a> &raquo;</li>
          <li class="nav-item nav-item-1"><a href="../../index.html" >Module code</a> &raquo;</li>
          <li class="nav-item nav-item-2"><a href="../../GPy.html" >GPy</a> &raquo;</li> 
      </ul>
    </div>
    <div class="footer" role="contentinfo">
        &copy; Copyright 2013, Author.
      Created using <a href="http://sphinx-doc.org/">Sphinx</a> 1.3.1.
    </div>
  </body>
</html>