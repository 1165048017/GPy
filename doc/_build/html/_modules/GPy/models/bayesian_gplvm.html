<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">


<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    
    <title>GPy.models.bayesian_gplvm &mdash; GPy  documentation</title>
    
    <link rel="stylesheet" href="../../../_static//default.css" type="text/css" />
    <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
    
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    '../../../',
        VERSION:     '',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true
      };
    </script>
    <script type="text/javascript" src="../../../_static/jquery.js"></script>
    <script type="text/javascript" src="../../../_static/underscore.js"></script>
    <script type="text/javascript" src="../../../_static/doctools.js"></script>
    <link rel="top" title="GPy  documentation" href="../../../index.html" />
    <link rel="up" title="GPy" href="../../GPy.html" /> 
  </head>
  <body role="document">
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../../../genindex.html" title="General Index"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="../../../py-modindex.html" title="Python Module Index"
             >modules</a> |</li>
        <li class="nav-item nav-item-0"><a href="../../../index.html">GPy  documentation</a> &raquo;</li>
          <li class="nav-item nav-item-1"><a href="../../index.html" >Module code</a> &raquo;</li>
          <li class="nav-item nav-item-2"><a href="../../GPy.html" accesskey="U">GPy</a> &raquo;</li> 
      </ul>
    </div>  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <h1>Source code for GPy.models.bayesian_gplvm</h1><div class="highlight"><pre>
<span class="c"># Copyright (c) 2012 - 2014 the GPy Austhors (see AUTHORS.txt)</span>
<span class="c"># Licensed under the BSD 3-clause license (see LICENSE.txt)</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">..</span> <span class="kn">import</span> <span class="n">kern</span>
<span class="kn">from</span> <span class="nn">..core.sparse_gp_mpi</span> <span class="kn">import</span> <span class="n">SparseGP_MPI</span>
<span class="kn">from</span> <span class="nn">..likelihoods</span> <span class="kn">import</span> <span class="n">Gaussian</span>
<span class="kn">from</span> <span class="nn">..core.parameterization.variational</span> <span class="kn">import</span> <span class="n">NormalPosterior</span><span class="p">,</span> <span class="n">NormalPrior</span>
<span class="kn">from</span> <span class="nn">..inference.latent_function_inference.var_dtc_parallel</span> <span class="kn">import</span> <span class="n">VarDTC_minibatch</span>
<span class="kn">import</span> <span class="nn">logging</span>

<div class="viewcode-block" id="BayesianGPLVM"><a class="viewcode-back" href="../../../GPy.models.html#GPy.models.bayesian_gplvm.BayesianGPLVM">[docs]</a><span class="k">class</span> <span class="nc">BayesianGPLVM</span><span class="p">(</span><span class="n">SparseGP_MPI</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Bayesian Gaussian Process Latent Variable Model</span>

<span class="sd">    :param Y: observed data (np.ndarray) or GPy.likelihood</span>
<span class="sd">    :type Y: np.ndarray| GPy.likelihood instance</span>
<span class="sd">    :param input_dim: latent dimensionality</span>
<span class="sd">    :type input_dim: int</span>
<span class="sd">    :param init: initialisation method for the latent space</span>
<span class="sd">    :type init: &#39;PCA&#39;|&#39;random&#39;</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">input_dim</span><span class="p">,</span> <span class="n">X</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">X_variance</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">init</span><span class="o">=</span><span class="s">&#39;PCA&#39;</span><span class="p">,</span> <span class="n">num_inducing</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
                 <span class="n">Z</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">kernel</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">inference_method</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">likelihood</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
                 <span class="n">name</span><span class="o">=</span><span class="s">&#39;bayesian gplvm&#39;</span><span class="p">,</span> <span class="n">mpi_comm</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">normalizer</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
                 <span class="n">missing_data</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">stochastic</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">batchsize</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">logger</span> <span class="o">=</span> <span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">__class__</span><span class="o">.</span><span class="n">__name__</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">X</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
            <span class="kn">from</span> <span class="nn">..util.initialization</span> <span class="kn">import</span> <span class="n">initialize_latent</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s">&quot;initializing latent space X with method {}&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">init</span><span class="p">))</span>
            <span class="n">X</span><span class="p">,</span> <span class="n">fracs</span> <span class="o">=</span> <span class="n">initialize_latent</span><span class="p">(</span><span class="n">init</span><span class="p">,</span> <span class="n">input_dim</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">fracs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">input_dim</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">init</span> <span class="o">=</span> <span class="n">init</span>

        <span class="k">if</span> <span class="n">X_variance</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s">&quot;initializing latent space variance ~ uniform(0,.1)&quot;</span><span class="p">)</span>
            <span class="n">X_variance</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="o">.</span><span class="mi">1</span><span class="p">,</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">Z</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s">&quot;initializing inducing inputs&quot;</span><span class="p">)</span>
            <span class="n">Z</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">permutation</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">copy</span><span class="p">())[:</span><span class="n">num_inducing</span><span class="p">]</span>
        <span class="k">assert</span> <span class="n">Z</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

        <span class="k">if</span> <span class="n">kernel</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s">&quot;initializing kernel RBF&quot;</span><span class="p">)</span>
            <span class="n">kernel</span> <span class="o">=</span> <span class="n">kern</span><span class="o">.</span><span class="n">RBF</span><span class="p">(</span><span class="n">input_dim</span><span class="p">,</span> <span class="n">lengthscale</span><span class="o">=</span><span class="mf">1.</span><span class="o">/</span><span class="n">fracs</span><span class="p">,</span> <span class="n">ARD</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span> <span class="c">#+ kern.Bias(input_dim) + kern.White(input_dim)</span>

        <span class="k">if</span> <span class="n">likelihood</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
            <span class="n">likelihood</span> <span class="o">=</span> <span class="n">Gaussian</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">variational_prior</span> <span class="o">=</span> <span class="n">NormalPrior</span><span class="p">()</span>
        <span class="n">X</span> <span class="o">=</span> <span class="n">NormalPosterior</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">X_variance</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">inference_method</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">mpi_comm</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
                <span class="n">inference_method</span> <span class="o">=</span> <span class="n">VarDTC_minibatch</span><span class="p">(</span><span class="n">mpi_comm</span><span class="o">=</span><span class="n">mpi_comm</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="kn">from</span> <span class="nn">..inference.latent_function_inference.var_dtc</span> <span class="kn">import</span> <span class="n">VarDTC</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="s">&quot;creating inference_method var_dtc&quot;</span><span class="p">)</span>
                <span class="n">inference_method</span> <span class="o">=</span> <span class="n">VarDTC</span><span class="p">(</span><span class="n">limit</span><span class="o">=</span><span class="mi">1</span> <span class="k">if</span> <span class="ow">not</span> <span class="n">missing_data</span> <span class="k">else</span> <span class="n">Y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">inference_method</span><span class="p">,</span><span class="n">VarDTC_minibatch</span><span class="p">):</span>
            <span class="n">inference_method</span><span class="o">.</span><span class="n">mpi_comm</span> <span class="o">=</span> <span class="n">mpi_comm</span>

        <span class="nb">super</span><span class="p">(</span><span class="n">BayesianGPLVM</span><span class="p">,</span><span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">__init__</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">Z</span><span class="p">,</span> <span class="n">kernel</span><span class="p">,</span> <span class="n">likelihood</span><span class="o">=</span><span class="n">likelihood</span><span class="p">,</span>
                                           <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span> <span class="n">inference_method</span><span class="o">=</span><span class="n">inference_method</span><span class="p">,</span>
                                           <span class="n">normalizer</span><span class="o">=</span><span class="n">normalizer</span><span class="p">,</span> <span class="n">mpi_comm</span><span class="o">=</span><span class="n">mpi_comm</span><span class="p">,</span>
                                           <span class="n">variational_prior</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">variational_prior</span><span class="p">,</span>
                                           <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">link_parameter</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">X</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<div class="viewcode-block" id="BayesianGPLVM.set_X_gradients"><a class="viewcode-back" href="../../../GPy.models.html#GPy.models.bayesian_gplvm.BayesianGPLVM.set_X_gradients">[docs]</a>    <span class="k">def</span> <span class="nf">set_X_gradients</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">X_grad</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Set the gradients of the posterior distribution of X in its specific form.&quot;&quot;&quot;</span>
        <span class="n">X</span><span class="o">.</span><span class="n">mean</span><span class="o">.</span><span class="n">gradient</span><span class="p">,</span> <span class="n">X</span><span class="o">.</span><span class="n">variance</span><span class="o">.</span><span class="n">gradient</span> <span class="o">=</span> <span class="n">X_grad</span>
</div>
<div class="viewcode-block" id="BayesianGPLVM.get_X_gradients"><a class="viewcode-back" href="../../../GPy.models.html#GPy.models.bayesian_gplvm.BayesianGPLVM.get_X_gradients">[docs]</a>    <span class="k">def</span> <span class="nf">get_X_gradients</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Get the gradients of the posterior distribution of X in its specific form.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">X</span><span class="o">.</span><span class="n">mean</span><span class="o">.</span><span class="n">gradient</span><span class="p">,</span> <span class="n">X</span><span class="o">.</span><span class="n">variance</span><span class="o">.</span><span class="n">gradient</span>
</div>
<div class="viewcode-block" id="BayesianGPLVM.parameters_changed"><a class="viewcode-back" href="../../../GPy.models.html#GPy.models.bayesian_gplvm.BayesianGPLVM.parameters_changed">[docs]</a>    <span class="k">def</span> <span class="nf">parameters_changed</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">BayesianGPLVM</span><span class="p">,</span><span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">parameters_changed</span><span class="p">()</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">inference_method</span><span class="p">,</span> <span class="n">VarDTC_minibatch</span><span class="p">):</span>
            <span class="k">return</span>        

        <span class="n">kl_fctr</span> <span class="o">=</span> <span class="mf">1.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_log_marginal_likelihood</span> <span class="o">-=</span> <span class="n">kl_fctr</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">variational_prior</span><span class="o">.</span><span class="n">KL_divergence</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">X</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">X</span><span class="o">.</span><span class="n">mean</span><span class="o">.</span><span class="n">gradient</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">X</span><span class="o">.</span><span class="n">variance</span><span class="o">.</span><span class="n">gradient</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">kern</span><span class="o">.</span><span class="n">gradients_qX_expectations</span><span class="p">(</span>
                                            <span class="n">variational_posterior</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">X</span><span class="p">,</span>
                                            <span class="n">Z</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">Z</span><span class="p">,</span>
                                            <span class="n">dL_dpsi0</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">grad_dict</span><span class="p">[</span><span class="s">&#39;dL_dpsi0&#39;</span><span class="p">],</span>
                                            <span class="n">dL_dpsi1</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">grad_dict</span><span class="p">[</span><span class="s">&#39;dL_dpsi1&#39;</span><span class="p">],</span>
                                            <span class="n">dL_dpsi2</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">grad_dict</span><span class="p">[</span><span class="s">&#39;dL_dpsi2&#39;</span><span class="p">])</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">variational_prior</span><span class="o">.</span><span class="n">update_gradients_KL</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">X</span><span class="p">)</span>


        <span class="c">#super(BayesianGPLVM, self).parameters_changed()</span>
        <span class="c">#self._log_marginal_likelihood -= self.variational_prior.KL_divergence(self.X)</span>

        <span class="c">#self.X.mean.gradient, self.X.variance.gradient = self.kern.gradients_qX_expectations(variational_posterior=self.X, Z=self.Z, dL_dpsi0=self.grad_dict[&#39;dL_dpsi0&#39;], dL_dpsi1=self.grad_dict[&#39;dL_dpsi1&#39;], dL_dpsi2=self.grad_dict[&#39;dL_dpsi2&#39;])</span>

        <span class="c"># This is testing code -------------------------</span>
<span class="c">#         i = np.random.randint(self.X.shape[0])</span>
<span class="c">#         X_ = self.X.mean</span>
<span class="c">#         which = np.sqrt(((X_ - X_[i:i+1])**2).sum(1)).argsort()&gt;(max(0, self.X.shape[0]-51))</span>
<span class="c">#         _, _, grad_dict = self.inference_method.inference(self.kern, self.X[which], self.Z, self.likelihood, self.Y[which], self.Y_metadata)</span>
<span class="c">#         grad = self.kern.gradients_qX_expectations(variational_posterior=self.X[which], Z=self.Z, dL_dpsi0=grad_dict[&#39;dL_dpsi0&#39;], dL_dpsi1=grad_dict[&#39;dL_dpsi1&#39;], dL_dpsi2=grad_dict[&#39;dL_dpsi2&#39;])</span>
<span class="c">#</span>
<span class="c">#         self.X.mean.gradient[:] = 0</span>
<span class="c">#         self.X.variance.gradient[:] = 0</span>
<span class="c">#         self.X.mean.gradient[which] = grad[0]</span>
<span class="c">#         self.X.variance.gradient[which] = grad[1]</span>

        <span class="c"># update for the KL divergence</span>
<span class="c">#         self.variational_prior.update_gradients_KL(self.X, which)</span>
        <span class="c"># -----------------------------------------------</span>

        <span class="c"># update for the KL divergence</span>
        <span class="c">#self.variational_prior.update_gradients_KL(self.X)</span>
</div>
<div class="viewcode-block" id="BayesianGPLVM.plot_latent"><a class="viewcode-back" href="../../../GPy.models.html#GPy.models.bayesian_gplvm.BayesianGPLVM.plot_latent">[docs]</a>    <span class="k">def</span> <span class="nf">plot_latent</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">which_indices</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
                <span class="n">resolution</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s">&#39;o&#39;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span>
                <span class="n">fignum</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">plot_inducing</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">legend</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
                <span class="n">plot_limits</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
                <span class="n">aspect</span><span class="o">=</span><span class="s">&#39;auto&#39;</span><span class="p">,</span> <span class="n">updates</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">predict_kwargs</span><span class="o">=</span><span class="p">{},</span> <span class="n">imshow_kwargs</span><span class="o">=</span><span class="p">{}):</span>
        <span class="kn">import</span> <span class="nn">sys</span>
        <span class="k">assert</span> <span class="s">&quot;matplotlib&quot;</span> <span class="ow">in</span> <span class="n">sys</span><span class="o">.</span><span class="n">modules</span><span class="p">,</span> <span class="s">&quot;matplotlib package has not been imported.&quot;</span>
        <span class="kn">from</span> <span class="nn">..plotting.matplot_dep</span> <span class="kn">import</span> <span class="n">dim_reduction_plots</span>

        <span class="k">return</span> <span class="n">dim_reduction_plots</span><span class="o">.</span><span class="n">plot_latent</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">which_indices</span><span class="p">,</span>
                <span class="n">resolution</span><span class="p">,</span> <span class="n">ax</span><span class="p">,</span> <span class="n">marker</span><span class="p">,</span> <span class="n">s</span><span class="p">,</span>
                <span class="n">fignum</span><span class="p">,</span> <span class="n">plot_inducing</span><span class="p">,</span> <span class="n">legend</span><span class="p">,</span>
                <span class="n">plot_limits</span><span class="p">,</span> <span class="n">aspect</span><span class="p">,</span> <span class="n">updates</span><span class="p">,</span> <span class="n">predict_kwargs</span><span class="p">,</span> <span class="n">imshow_kwargs</span><span class="p">)</span>
</div>
<div class="viewcode-block" id="BayesianGPLVM.do_test_latents"><a class="viewcode-back" href="../../../GPy.models.html#GPy.models.bayesian_gplvm.BayesianGPLVM.do_test_latents">[docs]</a>    <span class="k">def</span> <span class="nf">do_test_latents</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">Y</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Compute the latent representation for a set of new points Y</span>

<span class="sd">        Notes:</span>
<span class="sd">        This will only work with a univariate Gaussian likelihood (for now)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">N_test</span> <span class="o">=</span> <span class="n">Y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">input_dim</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">Z</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

        <span class="n">means</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">N_test</span><span class="p">,</span> <span class="n">input_dim</span><span class="p">))</span>
        <span class="n">covars</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">N_test</span><span class="p">,</span> <span class="n">input_dim</span><span class="p">))</span>

        <span class="n">dpsi0</span> <span class="o">=</span> <span class="o">-</span><span class="mf">0.5</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_dim</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">likelihood</span><span class="o">.</span><span class="n">variance</span>
        <span class="n">dpsi2</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">grad_dict</span><span class="p">[</span><span class="s">&#39;dL_dpsi2&#39;</span><span class="p">][</span><span class="mi">0</span><span class="p">][</span><span class="bp">None</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:]</span> <span class="c"># TODO: this may change if we ignore het. likelihoods</span>
        <span class="n">V</span> <span class="o">=</span> <span class="n">Y</span><span class="o">/</span><span class="bp">self</span><span class="o">.</span><span class="n">likelihood</span><span class="o">.</span><span class="n">variance</span>

        <span class="c">#compute CPsi1V</span>
        <span class="c">#if self.Cpsi1V is None:</span>
        <span class="c">#    psi1V = np.dot(self.psi1.T, self.likelihood.V)</span>
        <span class="c">#    tmp, _ = linalg.dtrtrs(self._Lm, np.asfortranarray(psi1V), lower=1, trans=0)</span>
        <span class="c">#    tmp, _ = linalg.dpotrs(self.LB, tmp, lower=1)</span>
        <span class="c">#    self.Cpsi1V, _ = linalg.dtrtrs(self._Lm, tmp, lower=1, trans=1)</span>

        <span class="n">dpsi1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">posterior</span><span class="o">.</span><span class="n">woodbury_vector</span><span class="p">,</span> <span class="n">V</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>

        <span class="c">#start = np.zeros(self.input_dim * 2)</span>


        <span class="kn">from</span> <span class="nn">scipy.optimize</span> <span class="kn">import</span> <span class="n">minimize</span>

        <span class="k">for</span> <span class="n">n</span><span class="p">,</span> <span class="n">dpsi1_n</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">dpsi1</span><span class="o">.</span><span class="n">T</span><span class="p">[:,</span> <span class="p">:,</span> <span class="bp">None</span><span class="p">]):</span>
            <span class="n">args</span> <span class="o">=</span> <span class="p">(</span><span class="n">input_dim</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">kern</span><span class="o">.</span><span class="n">copy</span><span class="p">(),</span> <span class="bp">self</span><span class="o">.</span><span class="n">Z</span><span class="p">,</span> <span class="n">dpsi0</span><span class="p">,</span> <span class="n">dpsi1_n</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">dpsi2</span><span class="p">)</span>
            <span class="n">res</span> <span class="o">=</span> <span class="n">minimize</span><span class="p">(</span><span class="n">latent_cost_and_grad</span><span class="p">,</span> <span class="n">jac</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">x0</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">hstack</span><span class="p">((</span><span class="n">means</span><span class="p">[</span><span class="n">n</span><span class="p">],</span> <span class="n">covars</span><span class="p">[</span><span class="n">n</span><span class="p">])),</span> <span class="n">args</span><span class="o">=</span><span class="n">args</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="s">&#39;BFGS&#39;</span><span class="p">)</span>
            <span class="n">xopt</span> <span class="o">=</span> <span class="n">res</span><span class="o">.</span><span class="n">x</span>
            <span class="n">mu</span><span class="p">,</span> <span class="n">log_S</span> <span class="o">=</span> <span class="n">xopt</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">means</span><span class="p">[</span><span class="n">n</span><span class="p">]</span> <span class="o">=</span> <span class="n">mu</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
            <span class="n">covars</span><span class="p">[</span><span class="n">n</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">log_S</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>

        <span class="n">X</span> <span class="o">=</span> <span class="n">NormalPosterior</span><span class="p">(</span><span class="n">means</span><span class="p">,</span> <span class="n">covars</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">X</span>
</div>
<div class="viewcode-block" id="BayesianGPLVM.dmu_dX"><a class="viewcode-back" href="../../../GPy.models.html#GPy.models.bayesian_gplvm.BayesianGPLVM.dmu_dX">[docs]</a>    <span class="k">def</span> <span class="nf">dmu_dX</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">Xnew</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Calculate the gradient of the prediction at Xnew w.r.t Xnew.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">dmu_dX</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">Xnew</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">Z</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
            <span class="n">dmu_dX</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">kern</span><span class="o">.</span><span class="n">gradients_X</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">grad_dict</span><span class="p">[</span><span class="s">&#39;dL_dpsi1&#39;</span><span class="p">][</span><span class="n">i</span><span class="p">:</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="p">:],</span> <span class="n">Xnew</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">Z</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="p">:])</span>
        <span class="k">return</span> <span class="n">dmu_dX</span>
</div>
<div class="viewcode-block" id="BayesianGPLVM.dmu_dXnew"><a class="viewcode-back" href="../../../GPy.models.html#GPy.models.bayesian_gplvm.BayesianGPLVM.dmu_dXnew">[docs]</a>    <span class="k">def</span> <span class="nf">dmu_dXnew</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">Xnew</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Individual gradient of prediction at Xnew w.r.t. each sample in Xnew</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">gradients_X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">Xnew</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_inducing</span><span class="p">))</span>
        <span class="n">ones</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">Z</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
            <span class="n">gradients_X</span><span class="p">[:,</span> <span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">kern</span><span class="o">.</span><span class="n">gradients_X</span><span class="p">(</span><span class="n">ones</span><span class="p">,</span> <span class="n">Xnew</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">Z</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="p">:])</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">gradients_X</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">grad_dict</span><span class="p">[</span><span class="s">&#39;dL_dpsi1&#39;</span><span class="p">])</span>
</div>
<div class="viewcode-block" id="BayesianGPLVM.plot_steepest_gradient_map"><a class="viewcode-back" href="../../../GPy.models.html#GPy.models.bayesian_gplvm.BayesianGPLVM.plot_steepest_gradient_map">[docs]</a>    <span class="k">def</span> <span class="nf">plot_steepest_gradient_map</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span> <span class="n">kwargs</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        See GPy.plotting.matplot_dep.dim_reduction_plots.plot_steepest_gradient_map</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="kn">import</span> <span class="nn">sys</span>
        <span class="k">assert</span> <span class="s">&quot;matplotlib&quot;</span> <span class="ow">in</span> <span class="n">sys</span><span class="o">.</span><span class="n">modules</span><span class="p">,</span> <span class="s">&quot;matplotlib package has not been imported.&quot;</span>
        <span class="kn">from</span> <span class="nn">..plotting.matplot_dep</span> <span class="kn">import</span> <span class="n">dim_reduction_plots</span>

        <span class="k">return</span> <span class="n">dim_reduction_plots</span><span class="o">.</span><span class="n">plot_steepest_gradient_map</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="o">*</span><span class="n">args</span><span class="p">,</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

</div></div>
<div class="viewcode-block" id="latent_cost_and_grad"><a class="viewcode-back" href="../../../GPy.models.html#GPy.models.bayesian_gplvm.latent_cost_and_grad">[docs]</a><span class="k">def</span> <span class="nf">latent_cost_and_grad</span><span class="p">(</span><span class="n">mu_S</span><span class="p">,</span> <span class="n">input_dim</span><span class="p">,</span> <span class="n">kern</span><span class="p">,</span> <span class="n">Z</span><span class="p">,</span> <span class="n">dL_dpsi0</span><span class="p">,</span> <span class="n">dL_dpsi1</span><span class="p">,</span> <span class="n">dL_dpsi2</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    objective function for fitting the latent variables for test points</span>
<span class="sd">    (negative log-likelihood: should be minimised!)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">mu</span> <span class="o">=</span> <span class="n">mu_S</span><span class="p">[:</span><span class="n">input_dim</span><span class="p">][</span><span class="bp">None</span><span class="p">]</span>
    <span class="n">log_S</span> <span class="o">=</span> <span class="n">mu_S</span><span class="p">[</span><span class="n">input_dim</span><span class="p">:][</span><span class="bp">None</span><span class="p">]</span>
    <span class="n">S</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">log_S</span><span class="p">)</span>

    <span class="n">X</span> <span class="o">=</span> <span class="n">NormalPosterior</span><span class="p">(</span><span class="n">mu</span><span class="p">,</span> <span class="n">S</span><span class="p">)</span>

    <span class="n">psi0</span> <span class="o">=</span> <span class="n">kern</span><span class="o">.</span><span class="n">psi0</span><span class="p">(</span><span class="n">Z</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span>
    <span class="n">psi1</span> <span class="o">=</span> <span class="n">kern</span><span class="o">.</span><span class="n">psi1</span><span class="p">(</span><span class="n">Z</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span>
    <span class="n">psi2</span> <span class="o">=</span> <span class="n">kern</span><span class="o">.</span><span class="n">psi2</span><span class="p">(</span><span class="n">Z</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span>

    <span class="n">lik</span> <span class="o">=</span> <span class="n">dL_dpsi0</span> <span class="o">*</span> <span class="n">psi0</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s">&#39;ij,kj-&gt;...&#39;</span><span class="p">,</span> <span class="n">dL_dpsi1</span><span class="p">,</span> <span class="n">psi1</span><span class="p">)</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s">&#39;ijk,lkj-&gt;...&#39;</span><span class="p">,</span> <span class="n">dL_dpsi2</span><span class="p">,</span> <span class="n">psi2</span><span class="p">)</span> <span class="o">-</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">mu</span><span class="p">)</span> <span class="o">+</span> <span class="n">S</span><span class="p">)</span> <span class="o">+</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">log_S</span><span class="p">)</span>

    <span class="n">dLdmu</span><span class="p">,</span> <span class="n">dLdS</span> <span class="o">=</span> <span class="n">kern</span><span class="o">.</span><span class="n">gradients_qX_expectations</span><span class="p">(</span><span class="n">dL_dpsi0</span><span class="p">,</span> <span class="n">dL_dpsi1</span><span class="p">,</span> <span class="n">dL_dpsi2</span><span class="p">,</span> <span class="n">Z</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span>
    <span class="n">dmu</span> <span class="o">=</span> <span class="n">dLdmu</span> <span class="o">-</span> <span class="n">mu</span>
    <span class="c"># dS = S0 + S1 + S2 -0.5 + .5/S</span>
    <span class="n">dlnS</span> <span class="o">=</span> <span class="n">S</span> <span class="o">*</span> <span class="p">(</span><span class="n">dLdS</span> <span class="o">-</span> <span class="mf">0.5</span><span class="p">)</span> <span class="o">+</span> <span class="o">.</span><span class="mi">5</span>

    <span class="k">return</span> <span class="o">-</span><span class="n">lik</span><span class="p">,</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">hstack</span><span class="p">((</span><span class="n">dmu</span><span class="o">.</span><span class="n">flatten</span><span class="p">(),</span> <span class="n">dlnS</span><span class="o">.</span><span class="n">flatten</span><span class="p">()))</span></div>
</pre></div>

          </div>
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<div id="searchbox" style="display: none" role="search">
  <h3>Quick search</h3>
    <form class="search" action="../../../search.html" method="get">
      <input type="text" name="q" />
      <input type="submit" value="Go" />
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
    <p class="searchtip" style="font-size: 90%">
    Enter search terms or a module, class or function name.
    </p>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../../../genindex.html" title="General Index"
             >index</a></li>
        <li class="right" >
          <a href="../../../py-modindex.html" title="Python Module Index"
             >modules</a> |</li>
        <li class="nav-item nav-item-0"><a href="../../../index.html">GPy  documentation</a> &raquo;</li>
          <li class="nav-item nav-item-1"><a href="../../index.html" >Module code</a> &raquo;</li>
          <li class="nav-item nav-item-2"><a href="../../GPy.html" >GPy</a> &raquo;</li> 
      </ul>
    </div>
    <div class="footer" role="contentinfo">
        &copy; Copyright 2013, Author.
      Created using <a href="http://sphinx-doc.org/">Sphinx</a> 1.3.1.
    </div>
  </body>
</html>